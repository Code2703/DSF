{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from config import GOOGLE_PATH, WEATHER_PATH, POP_PATH, FLU_PATH\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "flu = pd.read_csv('../data/INFLUENZA_sentinella/data.csv')\n",
    "weather = pd.read_csv('../data/weather/reg_weather.csv')\n",
    "google_flu = pd.read_csv('../data/google_search_trend/reg_google_grippe.csv')\n",
    "google_symptoms = pd.read_csv('../data/google_search_trend/reg_google_fieber_husten.csv')\n",
    "pop = pd.read_csv('../data/pop_data_cantons/weekly_imputed_pop_data_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data Preparation</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flu.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe for country-level observations, no differentiation between sex or age\n",
    "flu_CH = flu.query('georegion_type == \"country\" and agegroup == \"all\" and sex == \"all\"').copy()\n",
    "\n",
    "# Create dataframe for regional observations, no differentiation between sex or age\n",
    "flu_reg = flu.query('georegion_type == \"sentinella_region\" and agegroup == \"all\" and sex == \"all\"').copy()\n",
    "\n",
    "# Drop unknown region entries from flu_region using mask\n",
    "flu_reg = flu_reg[~(flu_reg['georegion'] == 'unknown')]\n",
    "\n",
    "# Select columns required for analysis\n",
    "selected_cols = ['temporal', 'georegion', 'popExtrapolation', 'incValue', 'value']\n",
    "flu_reg = flu_reg[selected_cols]\n",
    "flu_reg.rename(columns={'popExtrapolation':'flu_cases'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align time-indexes of google data and flu data\n",
    "with open('date_dict.json', 'r') as f:\n",
    "    date_dict = json.load(f)\n",
    "\n",
    "google_flu['Woche'] = google_flu['Woche'].apply(lambda x: date_dict[x])\n",
    "google_symptoms['Woche'] = google_symptoms['Woche'].apply(lambda x: date_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = weather.date.values\n",
    "# Convert dates to 'YYYY-Www' ISO week format\n",
    "iso_week_dates = [datetime.strptime(date, '%Y-%m-%d').isocalendar()[:2] for date in dates]\n",
    "iso_week_dates = [f'{year}-W{week:02d}' for year, week in iso_week_dates]\n",
    "weather['date'] = iso_week_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape google_flu from wide to long\n",
    "google_flu = google_flu.melt(id_vars=['Woche'], var_name='region_query', value_name='search_activity')\n",
    "\n",
    "# Separate region and query information from header into separate rows\n",
    "google_flu['region'] = google_flu['region_query'].apply(lambda x: \"_\".join(x.split('_')[:2]))\n",
    "google_flu['query'] = google_flu['region_query'].apply(lambda x: \"_\".join(x.split('_')[2:]))\n",
    "google_flu.drop(columns='region_query', inplace=True) # Drop superfluous region_query column\n",
    "\n",
    "# Reshape dataframe to get separate columns for each variable\n",
    "google_flu = google_flu.pivot(index=['Woche', 'region'], columns='query', values='search_activity').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape google_flu from wide to long\n",
    "google_symptoms = google_symptoms.melt(id_vars=['Woche'], var_name='region_query', value_name='search_activity')\n",
    "\n",
    "# Separate region and query information from header into separate rows\n",
    "google_symptoms['region'] = google_symptoms['region_query'].apply(lambda x: \"_\".join(x.split('_')[:2]))\n",
    "google_symptoms['query'] = google_symptoms['region_query'].apply(lambda x: \"_\".join(x.split('_')[2:]))\n",
    "google_symptoms.drop(columns='region_query', inplace=True) # Drop superfluous region_query column\n",
    "\n",
    "# Reshape dataframe to get separate columns for each variable\n",
    "google_symptoms = google_symptoms.pivot(index=['Woche', 'region'], columns='query', values='search_activity').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.merge(flu_reg, weather, how='left', left_on=['temporal', 'georegion'], right_on=['date', 'region']).sort_values(by=['georegion', 'temporal'])\n",
    "merged_google = pd.merge(google_flu, google_symptoms, how='inner', on=['region', 'Woche'])\n",
    "merged_data = pd.merge(merged_data, merged_google, how='left', left_on=['georegion', 'temporal'], right_on=['region', 'Woche'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.drop(columns=['region_x', 'region_y', 'date', 'Woche'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Functions taken from: https://stackoverflow.com/questions/304256/whats-the-best-way-to-find-the-inverse-of-datetime-isocalendar\n",
    "def iso_year_start(iso_year):\n",
    "    \"The gregorian calendar date of the first day of the given ISO year\"\n",
    "    fourth_jan = datetime.date(iso_year, 1, 4)\n",
    "    delta = datetime.timedelta(fourth_jan.isoweekday()-1)\n",
    "    return fourth_jan - delta \n",
    "\n",
    "def iso_to_gregorian(iso_year, iso_week, iso_day):\n",
    "    \"Gregorian calendar date for the given ISO year, week and day\"\n",
    "    year_start = iso_year_start(iso_year)\n",
    "    return year_start + datetime.timedelta(days=iso_day-1, weeks=iso_week-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the regex pattern\n",
    "pattern = r'W(\\d{1,2})'\n",
    "\n",
    "# Extract week number and year and convert to integer\n",
    "merged_data['week_number'] = merged_data['temporal'].str.extract(pattern).astype(int)\n",
    "\n",
    "merged_data['year'] = merged_data['temporal'].apply(lambda x: x.split('-')[0])\n",
    "merged_data['year'] = pd.to_numeric(merged_data['year'])\n",
    "\n",
    "# Convert from iso-calendar week to dates\n",
    "merged_data['date'] = list(map(lambda year, week: iso_to_gregorian(year, week, 4), merged_data['year'], merged_data['week_number']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â merged_data.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values in March 2020 linearly\n",
    "merged_data['incValue'].interpolate(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(6, figsize=(12, 12))\n",
    "for i in range(1, 7):\n",
    "    ax[i-1].plot(merged_data.loc[merged_data['georegion'] == f\"region_{i}\", 'incValue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "merged_data.loc[merged_data['georegion'] == \"region_5\", 'incValue'].hist()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# Assuming you have imported your dataset and stored it in merged_data\n",
    "\n",
    "# Create the PowerTransformer with 'yeo-johnson' method\n",
    "boxcox_transformer = PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "# Filter and select the data based on the 'georegion' condition and 'incValue' column\n",
    "filtered_data = merged_data.loc[merged_data['georegion'] == \"region_5\", 'incValue']\n",
    "\n",
    "# Fit and transform the filtered data\n",
    "transformed_data = boxcox_transformer.fit_transform(filtered_data.values.reshape(-1, 1))\n",
    "\n",
    "# Create a figure and axis for the transformed data\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.plot(transformed_data)\n",
    "ax1.set_title('Transformed Data')\n",
    "\n",
    "# Create a figure and axis for the histogram of transformed data\n",
    "fig2, ax2 = plt.subplots()\n",
    "ax2.hist(transformed_data, bins=30, alpha=0.5)\n",
    "ax2.set_title('Histogram of Transformed Data')\n",
    "\n",
    "# Show the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Feature Engineering</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neural_network import MLPRegressor\n",
    "# from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "# from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "# from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def autoregressive_iterative_forecast(model, initial_input, n_steps):\n",
    "#     \"\"\"\n",
    "#     Perform iterative forecasting using an autoregressive model.\n",
    "\n",
    "#     Args:\n",
    "#         model: Trained autoregressive model (e.g., MLPRegressor).\n",
    "#         initial_input: The initial input features (e.g., the last observation from the training set).\n",
    "#         n_steps: Number of future time steps to forecast.\n",
    "\n",
    "#     Returns:\n",
    "#         A list of forecasts, one for each future time step.\n",
    "#     \"\"\"\n",
    "#     current_input = initial_input.copy()\n",
    "#     forecasts = []\n",
    "\n",
    "#     for _ in range(n_steps):\n",
    "#         # Predict the next step\n",
    "#         next_step_pred = model.predict(current_input.reshape(1, -1))[0]\n",
    "#         forecasts.append(next_step_pred)\n",
    "\n",
    "#         # Update the current input to include the new prediction\n",
    "#         current_input = np.roll(current_input, -1)\n",
    "#         current_input[-1] = next_step_pred\n",
    "\n",
    "#     return np.array(forecasts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_lagged_features(df, column, number_of_lags=4, seasonal_lags=[52]):\n",
    "#     # Define a helper function to apply within each group\n",
    "#     def add_lags(group):\n",
    "#         # Create a dictionary for the lagged features\n",
    "#         lags = {f'lag_{i}': group[column].shift(i) for i in range(1, number_of_lags + 1)}\n",
    "#         for seasonal_lag in seasonal_lags:\n",
    "#             lags.update({f'seasonal_lag_{i}': group[column].shift(i) for i in range(seasonal_lag - 2, seasonal_lag + 3)})\n",
    "#         # Return the original column along with the lagged features\n",
    "#         return pd.concat([group[column], pd.DataFrame(lags, index=group.index)], axis=1)\n",
    "\n",
    "#     # Group by 'georegion' and apply the lagging within each group\n",
    "#     new_df = df.groupby('georegion', group_keys=False).apply(add_lags)\n",
    "\n",
    "#     # Reset index to flatten the dataframe structure after grouping\n",
    "#     new_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#     return new_df\n",
    "\n",
    "\n",
    "# data = merged_data.loc[(merged_data['georegion'] == \"region_5\") & (merged_data['date'].apply(lambda x: x.year < 2020))]\n",
    "\n",
    "# df_lagged = create_lagged_features(data, column='incValue')\n",
    "# df_lagged.dropna(inplace=True)\n",
    "# training_cols = [col for col in df_lagged.columns if 'lag_' in col]\n",
    "# X = df_lagged[training_cols]\n",
    "# y = df_lagged['incValue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_lagged_features(df, column, number_of_lags=4):\n",
    "#     # Define a helper function to apply within each group\n",
    "#     def add_lags(group):\n",
    "#         # Create a dictionary for the lagged features\n",
    "#         lags = {f'lag_{i}': group[column].shift(i) for i in range(1, number_of_lags + 1)}\n",
    "#         # Return the original column along with the lagged features\n",
    "#         return pd.concat([group[column], pd.DataFrame(lags, index=group.index)], axis=1)\n",
    "\n",
    "#     # Group by 'georegion' and apply the lagging within each group\n",
    "#     new_df = df.groupby('georegion', group_keys=False).apply(add_lags)\n",
    "\n",
    "#     # Reset index to flatten the dataframe structure after grouping\n",
    "#     new_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#     return new_df\n",
    "\n",
    "# data = merged_data.loc[(merged_data['georegion'] == \"region_5\") & (merged_data['date'].apply(lambda x: x.year < 2020))]\n",
    "\n",
    "# df_lagged = create_lagged_features(data, column='incValue')\n",
    "# df_lagged.dropna(inplace=True)\n",
    "# training_cols = [col for col in df_lagged.columns if 'lag_' in col]\n",
    "# X = df_lagged[training_cols]\n",
    "# y = df_lagged['incValue']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Backups of the first versions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = merged_data.loc[(merged_data['georegion'] == \"region_5\") & (merged_data['date'].apply(lambda x: x.year < 2020))]\n",
    "\n",
    "# # Define parameter configurations to assess\n",
    "# lags = 51 # Autoregressive lags to consider\n",
    "# hidden_layer_sizes = [(32,), (32, 32)]\n",
    "# alphas = [0.0001, 0.001, 0.01] # Regularization parameter\n",
    "# batch_sizes = [32]\n",
    "# learning_rates = [0.001, 0.01]\n",
    "# activations = ['relu']\n",
    "# seasonal = [[52]]\n",
    "\n",
    "# models_count = (lags-30) * len(hidden_layer_sizes) * len(alphas) * len(batch_sizes) * len(learning_rates) * len(activations) * len(seasonal)\n",
    "\n",
    "# i = 0\n",
    "# scores_df = pd.DataFrame(columns=['RMSE', 'lags', 'seasonal_lags', 'hidden_layers', 'alpha', 'batch_size', 'activation', 'learning_rate'])\n",
    "# # Grid search hyperparameter configurations\n",
    "# for lag in range(30, lags):\n",
    "#     for seasonal_lags in seasonal:\n",
    "#         # Create lagged features\n",
    "#         df_lagged = create_lagged_features(data, column='incValue', number_of_lags=lags, seasonal_lags=seasonal_lags)\n",
    "#         df_lagged.dropna(inplace=True)\n",
    "#         training_cols = [col for col in df_lagged.columns if 'lag_' in col]\n",
    "#         X = df_lagged[training_cols]\n",
    "#         y = df_lagged['incValue']\n",
    "\n",
    "#         # Split the data\n",
    "#         split = int(len(y) * 0.8)\n",
    "#         X_train, X_test = X[:split], X[split:]\n",
    "#         y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "#         # Scale the inputs\n",
    "#         scaler_X = MinMaxScaler()\n",
    "#         X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "#         X_test_scaled = scaler_X.transform(X_test)  # Use the same scaler as for X_train\n",
    "\n",
    "#         # Scale the target\n",
    "#         scaler_y = MinMaxScaler()\n",
    "#         y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).ravel()  # Reshape to 1D array\n",
    "#         y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1)).ravel()        # Reshape to 1D array\n",
    "\n",
    "#         # Keep track of configurations and cv scores\n",
    "\n",
    "#         for hidden_layers in hidden_layer_sizes:\n",
    "#             for learning_rate in learning_rates:\n",
    "#                 for activation in activations:\n",
    "#                     for alpha in alphas:\n",
    "#                         for batch_size in batch_sizes:\n",
    "#                             model = MLPRegressor(max_iter=1000, \n",
    "#                                                 random_state=42, \n",
    "#                                                 solver='adam', \n",
    "#                                                 activation=activation, \n",
    "#                                                 hidden_layer_sizes=hidden_layers, \n",
    "#                                                 alpha=alpha, \n",
    "#                                                 batch_size=batch_size, \n",
    "#                                                 learning_rate_init=learning_rate,\n",
    "#                                                 early_stopping=True, \n",
    "#                                                 validation_fraction=0.1, \n",
    "#                                                 n_iter_no_change=100,\n",
    "#                                                 tol=1e-3)\n",
    "#                             scores = []\n",
    "#                             tscv = TimeSeriesSplit(n_splits=3)\n",
    "#                             for train_index, val_index in tscv.split(X_train):\n",
    "#                                 # Create rolling windows\n",
    "#                                 X_train_cv, X_val = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "#                                 y_train_cv, y_val = y_train_scaled[train_index], y_train_scaled[val_index]\n",
    "                                \n",
    "#                                 # Fit model\n",
    "#                                 model.fit(X_train_cv, y_train_cv)\n",
    "                                \n",
    "#                                 predictions = autoregressive_iterative_forecast(model, X_train_cv[-1], len(y_val))\n",
    "                                \n",
    "#                                 rmse = mean_squared_error(y_val, predictions, squared=False)\n",
    "#                                 scores.append(rmse)\n",
    "                            \n",
    "#                             # Fill in parameters and score for each configuration \n",
    "#                             scores_df.loc[i, 'lags'] = lag\n",
    "#                             scores_df.loc[i, 'seasonal_lags'] = seasonal_lags\n",
    "#                             scores_df.loc[i, 'hidden_layers'] = hidden_layers\n",
    "#                             scores_df.loc[i, 'alpha'] = alpha\n",
    "#                             scores_df.loc[i, 'batch_size'] = batch_size\n",
    "#                             scores_df.loc[i, 'activation'] = activation\n",
    "#                             scores_df.loc[i, 'learning_rate'] = learning_rate\n",
    "#                             scores_df.loc[i, 'RMSE'] = np.mean(scores)\n",
    "#                             print(f'{i}/{models_count}: {(i/models_count)*100:.2f}%')\n",
    "#                             i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = merged_data.loc[(merged_data['georegion'] == \"region_5\") & (merged_data['date'].apply(lambda x: x.year < 2020))]\n",
    "\n",
    "# # Define parameter configurations to assess\n",
    "# lags = 51 # Autoregressive lags to consider\n",
    "# hidden_layer_sizes = [(32,), (32, 32)]\n",
    "# alphas = [0.0001, 0.001, 0.01] # Regularization parameter\n",
    "# batch_sizes = [32]\n",
    "# learning_rates = [0.001, 0.01]\n",
    "# activations = ['relu']\n",
    "# seasonal = [[52]]\n",
    "\n",
    "# models_count = (lags-30) * len(hidden_layer_sizes) * len(alphas) * len(batch_sizes) * len(learning_rates) * len(activations) * len(seasonal)\n",
    "\n",
    "# i = 0\n",
    "# scores_df = pd.DataFrame(columns=['RMSE', 'lags', 'seasonal_lags', 'hidden_layers', 'alpha', 'batch_size', 'activation', 'learning_rate'])\n",
    "# # Grid search hyperparameter configurations\n",
    "# for lag in range(30, lags):\n",
    "#     # Create lagged features\n",
    "#     df_lagged = create_lagged_features(data, column='incValue', number_of_lags=lags, seasonal_lags=seasonal_lags)\n",
    "#     df_lagged.dropna(inplace=True)\n",
    "#     training_cols = [col for col in df_lagged.columns if 'lag_' in col]\n",
    "#     X = df_lagged[training_cols]\n",
    "#     y = df_lagged['incValue']\n",
    "\n",
    "#     # Split the data\n",
    "#     split = int(len(y) * 0.8)\n",
    "#     X_train, X_test = X[:split], X[split:]\n",
    "#     y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "#     # Scale the inputs\n",
    "#     scaler_X = MinMaxScaler()\n",
    "#     X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "#     X_test_scaled = scaler_X.transform(X_test)  # Use the same scaler as for X_train\n",
    "\n",
    "#     # Scale the target\n",
    "#     scaler_y = MinMaxScaler()\n",
    "#     y_train_scaled = scaler_y.fit_transform(y_train.values.reshape(-1, 1)).ravel()  # Reshape to 1D array\n",
    "#     y_test_scaled = scaler_y.transform(y_test.values.reshape(-1, 1)).ravel()        # Reshape to 1D array\n",
    "\n",
    "#     # Keep track of configurations and cv scores\n",
    "\n",
    "#     for hidden_layers in hidden_layer_sizes:\n",
    "#         for learning_rate in learning_rates:\n",
    "#             for activation in activations:\n",
    "#                 for alpha in alphas:\n",
    "#                     for batch_size in batch_sizes:\n",
    "#                         model = MLPRegressor(max_iter=1000, \n",
    "#                                             random_state=42, \n",
    "#                                             solver='adam', \n",
    "#                                             activation=activation, \n",
    "#                                             hidden_layer_sizes=hidden_layers, \n",
    "#                                             alpha=alpha, \n",
    "#                                             batch_size=batch_size, \n",
    "#                                             learning_rate_init=learning_rate,\n",
    "#                                             early_stopping=True, \n",
    "#                                             validation_fraction=0.1, \n",
    "#                                             n_iter_no_change=100,\n",
    "#                                             tol=1e-3)\n",
    "#                         scores = []\n",
    "#                         tscv = TimeSeriesSplit(n_splits=3)\n",
    "#                         for train_index, val_index in tscv.split(X_train):\n",
    "#                             # Create rolling windows\n",
    "#                             X_train_cv, X_val = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "#                             y_train_cv, y_val = y_train_scaled[train_index], y_train_scaled[val_index]\n",
    "                            \n",
    "#                             # Fit model\n",
    "#                             model.fit(X_train_cv, y_train_cv)\n",
    "#                             predictions = []\n",
    "#                             for t in range(len(y_val)):\n",
    "#                                 one_step_prediction = autoregressive_iterative_forecast(model, X_train_cv[-1] if t == 0 else X_val[t-1], 1)\n",
    "#                                 predictions.append(one_step_prediction)\n",
    "                            \n",
    "#                             rmse = mean_squared_error(y_val, predictions, squared=False)\n",
    "#                             scores.append(rmse)\n",
    "                        \n",
    "#                         # Fill in parameters and score for each configuration \n",
    "#                         scores_df.loc[i, 'lags'] = lag\n",
    "#                         scores_df.loc[i, 'seasonal_lags'] = seasonal_lags\n",
    "#                         scores_df.loc[i, 'hidden_layers'] = hidden_layers\n",
    "#                         scores_df.loc[i, 'alpha'] = alpha\n",
    "#                         scores_df.loc[i, 'batch_size'] = batch_size\n",
    "#                         scores_df.loc[i, 'activation'] = activation\n",
    "#                         scores_df.loc[i, 'learning_rate'] = learning_rate\n",
    "#                         scores_df.loc[i, 'RMSE'] = np.mean(scores)\n",
    "#                         print(f'{i}/{models_count}: {(i/models_count)*100:.2f}%')\n",
    "#                         i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE BACKUP\n",
    "# def autoregressive_iterative_forecast(model, initial_input, n_steps):\n",
    "#     \"\"\"\n",
    "#     Perform iterative forecasting using an autoregressive model.\n",
    "\n",
    "#     Args:\n",
    "#         model: Trained autoregressive model (e.g., MLPRegressor).\n",
    "#         initial_input: The initial input features (e.g., the last observation from the training set).\n",
    "#         n_steps: Number of future time steps to forecast.\n",
    "\n",
    "#     Returns:\n",
    "#         A list of forecasts, one for each future time step.\n",
    "#     \"\"\"\n",
    "#     current_input = initial_input.copy()\n",
    "#     forecasts = []\n",
    "\n",
    "#     for _ in range(n_steps):\n",
    "#         # Predict the next step\n",
    "#         next_step_pred = model.predict(current_input.reshape(1, -1))[0]\n",
    "#         forecasts.append(next_step_pred)\n",
    "\n",
    "#         # Update the current input to include the new prediction\n",
    "#         current_input = np.roll(current_input, -1)\n",
    "#         current_input[-1] = next_step_pred\n",
    "\n",
    "#     return np.array(forecasts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Modelling</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lagged_features(df, column, number_of_lags, seasonal_lags=None):\n",
    "    # Copy the original DataFrame to avoid modifying it\n",
    "    df_lagged = df.copy()\n",
    "\n",
    "    # Generate regular lagged features\n",
    "    for lag in range(1, number_of_lags + 1):\n",
    "        df_lagged[f'lag_{lag}'] = df_lagged[column].shift(lag)\n",
    "\n",
    "    # Generate seasonal lags\n",
    "    if seasonal_lags is not None:\n",
    "        for season_lag in seasonal_lags:\n",
    "            df_lagged[f'seasonal_lag_{season_lag}_helper'] = df_lagged[column].shift(season_lag-1)\n",
    "            df_lagged[f'seasonal_lag_{season_lag}'] = df_lagged[column].shift(season_lag)\n",
    "\n",
    "    return df_lagged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoregressive_iterative_forecast(model, initial_input, seasonal_input, n_steps):\n",
    "    \"\"\"\n",
    "    Perform iterative forecasting using an autoregressive model.\n",
    "\n",
    "    Args:\n",
    "        model: Trained autoregressive model (e.g., MLPRegressor).\n",
    "        initial_input: The initial input features (e.g., the last observation from the training set).\n",
    "        n_steps: Number of future time steps to forecast.\n",
    "\n",
    "    Returns:\n",
    "        A list of forecasts, one for each future time step.\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    current_input = initial_input.copy()\n",
    "    current_input = np.array(current_input)\n",
    "    seasonal_input = np.array(seasonal_input)\n",
    "    # print(f'Seasonal lags: {seasonal_input}')\n",
    "    forecasts = []\n",
    "\n",
    "    for _ in range(n_steps):\n",
    "        # Predict the next step\n",
    "        # print(f'Current_input start: {current_input}\\n')\n",
    "        # print(len(current_input))\n",
    "        next_step_pred = model.predict(current_input.reshape(1, -1))[0]\n",
    "        # print(f'\\nWeek {i+1}\\n')  \n",
    "        # print(f'Pred: {next_step_pred}')  \n",
    "        forecasts.append(next_step_pred)\n",
    "        # print(f'Forecasts: {forecasts}')\n",
    "        \n",
    "        # Update the current input to include the new prediction\n",
    "        # Roll all lags except the last one (seasonal lag)\n",
    "        current_input[:-1] = np.roll(current_input[:-1], 1)\n",
    "        # print(f'Current_input after roll: {current_input}\\n')\n",
    "        current_input[0] = next_step_pred\n",
    "        \n",
    "        # Update the seasonal lag (52-period lag)\n",
    "        if i < 52:\n",
    "            # Use the actual seasonal lag value for the first 52 weeks\n",
    "            current_input[-1] = seasonal_input[i]\n",
    "        else:\n",
    "            # Use forecasted value for the seasonal lag after 52 weeks\n",
    "            current_input[-1] = forecasts[i - 52]\n",
    "        # This needs to be handled based on your specific logic\n",
    "        # For example, fetching the value from 52 periods ago or some other logic\n",
    "        \n",
    "        # print(f'Current_input with next_step_pred: {current_input}')\n",
    "        # print(f'\\n {60*\"#\"}\\n')\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return np.array(forecasts)\n",
    "\n",
    "# training_cols = [col for col in df_lagged.columns if ('lag_' in col) and ('_helper' not in col)]\n",
    "# X_train_cv_scaled = df_lagged[training_cols]\n",
    "# print(X_train_cv_scaled.iloc[0], f'Length:{len(X_train_cv_scaled.iloc[0])}')\n",
    "# print(df_lagged['seasonal_lag_52_helper'].iloc[:52])\n",
    "# prediction = autoregressive_iterative_forecast(model, X_train_cv_scaled.iloc[0], df_lagged[['seasonal_lag_52_helper']], 53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define parameter configurations to assess\n",
    "lags = 52 # Autoregressive lags to consider\n",
    "hidden_layer_size = (32, 32)\n",
    "alpha = 0.001 # Regularization parameter\n",
    "batch_size = 32\n",
    "learning_rate = 0.01\n",
    "activations = 'relu'\n",
    "seasonal = [52]\n",
    "models_count = lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "data = merged_data.loc[(merged_data['georegion'] == \"region_5\") & (merged_data['date'].apply(lambda x: x.year < 2020))]\n",
    "# Split the data\n",
    "y = data['incValue']\n",
    "split = int(len(y) * 0.8)\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "i = 0\n",
    "scores_df = pd.DataFrame(columns=['RMSE', 'lags', 'seasonal_lags', 'hidden_layers', 'alpha', 'batch_size', 'activation', 'learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search hyperparameter configurations\n",
    "for lag in range(1, lags):\n",
    "    \n",
    "    # Keep track of configurations and cv scores\n",
    "    model = MLPRegressor(max_iter=1000, \n",
    "                        random_state=42, \n",
    "                        solver='adam', \n",
    "                        activation=activations, \n",
    "                        hidden_layer_sizes=hidden_layer_size, \n",
    "                        alpha=alpha, \n",
    "                        batch_size=batch_size, \n",
    "                        learning_rate_init=learning_rate)\n",
    "    scores = []\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    fold = 0\n",
    "    \n",
    "    # Create lagged features based on the whole y_train\n",
    "    df_lagged = create_lagged_features(pd.DataFrame(y_train, columns=['incValue']), column='incValue', number_of_lags=lag, seasonal_lags=seasonal)\n",
    "    df_lagged.dropna(inplace=True)\n",
    "    \n",
    "    training_cols = [col for col in df_lagged.columns if ('lag_' in col) and ('_helper' not in col)]\n",
    "    X = df_lagged[training_cols]\n",
    "    X_seasonal = df_lagged['seasonal_lag_52_helper']\n",
    "    y = df_lagged['incValue']\n",
    "    print(X)\n",
    "    print(y)\n",
    "    \n",
    "    for train_index, val_index in tscv.split(X):\n",
    "        \n",
    "        y_train_cv, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        X_train_cv, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "        X_seasonal_train, X_seasonal_val = X_seasonal.iloc[train_index], X_seasonal.iloc[val_index]\n",
    "\n",
    "        # Take the first row of X_train_cv (the oldest lags)\n",
    "        oldest_lags = X_train_cv.iloc[0, 1:].values.reshape(1, -1)\n",
    "\n",
    "        # Concatenate y_train_cv with the oldest lags\n",
    "        combined_data = np.vstack((y_train_cv.values.reshape(-1, 1), oldest_lags.T))\n",
    "\n",
    "        # Fit the PowerTransformer and StandardScaler on the available lags in the training data (incl. lags in first row of lag df_train)\n",
    "        pt = PowerTransformer(method='yeo-johnson')\n",
    "        stdscaler = StandardScaler()\n",
    "        combined_data_scaled = pt.fit_transform(combined_data)\n",
    "        stdscaler.fit(combined_data_scaled)\n",
    "        \n",
    "        # Apply Transform to the entire y_train_cv\n",
    "        y_train_cv_transformed = pt.transform(y_train_cv.values.reshape(-1, 1)).flatten()\n",
    "        y_val_transformed = pt.transform(y_val.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "        # Apply the PowerTransformer to each lagged feature in X_train_cv and X_val\n",
    "        X_train_cv_transformed = X_train_cv.apply(lambda column: pt.transform(column.values.reshape(-1, 1)).flatten())\n",
    "        X_val_transformed = X_val.apply(lambda column: pt.transform(column.values.reshape(-1, 1)).flatten())\n",
    "        X_seasonal_train_trans = pt.transform(X_seasonal_train.values.reshape(-1, 1)).flatten()\n",
    "        X_seasonal_val_trans = pt.transform(X_seasonal_val.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "        \n",
    "        # Apply StandardScaler()\n",
    "        y_train_cv_scaled = stdscaler.transform(y_train_cv_transformed.reshape(-1, 1)).flatten()\n",
    "        y_val_scaled = stdscaler.transform(y_val_transformed.reshape(-1, 1)).flatten()\n",
    "        X_train_cv_scaled = X_train_cv_transformed.apply(lambda column: stdscaler.transform(column.values.reshape(-1, 1)).flatten())\n",
    "        X_val_scaled = X_val_transformed.apply(lambda column: stdscaler.transform(column.values.reshape(-1, 1)).flatten())\n",
    "        X_seasonal_train_scaled = stdscaler.transform(X_seasonal_train_trans.reshape(-1, 1)).flatten()\n",
    "        X_seasonal_val_scaled = stdscaler.transform(X_seasonal_val_trans.reshape(-1, 1)).flatten()\n",
    "\n",
    "        # Fit model\n",
    "        model.fit(X_train_cv_scaled, y_train_cv_scaled)\n",
    "        # loss_values = model.loss_curve_\n",
    "        \n",
    "        # Make iterative forecasts (NOTE: train and val splits are numpy arrays, seasonal helper columns necessary for updating of seasonal lag)\n",
    "        # print(f'X_val_scaled: {X_val_scaled}')\n",
    "        # print(f'X_val_scaled: {X_val_scaled.iloc[0]}')\n",
    "        prediction = autoregressive_iterative_forecast(model, X_val_scaled.iloc[0], X_seasonal_val_scaled, len(y_val_scaled))\n",
    "        y_hat_train = autoregressive_iterative_forecast(model, X_train_cv_scaled.iloc[0], X_seasonal_train_scaled, len(y_train_cv_scaled))\n",
    "        prediction = np.array(prediction).flatten()\n",
    "        y_hat_train = np.array(y_hat_train).flatten()\n",
    "\n",
    "        # NOTE: UNCOMMENT FOR PLOTS OF VALIDATION - Plot actual vs predicted values\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(len(y_train_cv_scaled)), y_train_cv_scaled, label='Training Actual', color='blue')\n",
    "        plt.plot(range(len(y_train_cv_scaled), len(y_train_cv_scaled) + len(y_val_scaled)), y_val_scaled, label='Validation Actual', color='blue')\n",
    "        plt.plot(range(len(y_train_cv_scaled), len(y_train_cv_scaled) + len(y_val_scaled)), prediction, label='Validation Predicted', color='red', linestyle='--')\n",
    "        plt.plot(range(len(y_train_cv_scaled)), y_hat_train, label='Train-Set Prediction', color='orange', linestyle='--')\n",
    "        \n",
    "        plt.title(f'Lag: {lag}; Fold {fold+1} Predictions vs Actual')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Scaled Value')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        # plt.plot(loss_values, label='Training Loss')\n",
    "        # # If you have validation loss, plot it here\n",
    "        # # plt.plot(validation_loss_values, label='Validation Loss')\n",
    "\n",
    "        # plt.title('Learning Curve')\n",
    "        # plt.xlabel('Epochs')\n",
    "        # plt.ylabel('Loss')\n",
    "        # plt.legend()\n",
    "        # plt.show()\n",
    "        \n",
    "        rmse = mean_squared_error(y_val_scaled, prediction, squared=False)\n",
    "\n",
    "        scores.append(rmse)\n",
    "        fold += 1\n",
    "    \n",
    "    # Fill in parameters and score for each configuration \n",
    "    scores_df.loc[i, 'lags'] = lag\n",
    "    scores_df.loc[i, 'seasonal_lags'] = seasonal\n",
    "    scores_df.loc[i, 'hidden_layers'] = hidden_layer_size\n",
    "    scores_df.loc[i, 'alpha'] = alpha\n",
    "    scores_df.loc[i, 'batch_size'] = batch_size\n",
    "    scores_df.loc[i, 'activation'] = activations\n",
    "    scores_df.loc[i, 'learning_rate'] = learning_rate\n",
    "    scores_df.loc[i, 'RMSE'] = np.mean(scores)\n",
    "    print(f'{i}/{models_count}: {(i/models_count)*100:.2f}%')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Backup --------------</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import PowerTransformer\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.neural_network import MLPRegressor\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "# from sklearn.model_selection import TimeSeriesSplit\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# # Define parameter configurations to assess\n",
    "# lags = 52 # Autoregressive lags to consider\n",
    "# hidden_layer_size = (32, 32)\n",
    "# alpha = 0.001 # Regularization parameter\n",
    "# batch_size = 32\n",
    "# learning_rate = 0.01\n",
    "# activations = 'relu'\n",
    "# seasonal = [52]\n",
    "# models_count = lags\n",
    "\n",
    "# # Extract data\n",
    "# data = merged_data.loc[(merged_data['georegion'] == \"region_5\") & (merged_data['date'].apply(lambda x: x.year < 2020))]\n",
    "# # Split the data\n",
    "# y = data['incValue']\n",
    "# split = int(len(y) * 0.8)\n",
    "# y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "# i = 0\n",
    "# scores_df = pd.DataFrame(columns=['RMSE', 'lags', 'seasonal_lags', 'hidden_layers', 'alpha', 'batch_size', 'activation', 'learning_rate'])\n",
    "# # Grid search hyperparameter configurations\n",
    "# for lag in range(1, lags):\n",
    "    \n",
    "#     # Apply Yeo-Johnson Transform\n",
    "#     pt = PowerTransformer(method='yeo-johnson')\n",
    "#     y_train_transformed = pt.fit_transform(y_train.values.reshape(-1, 1)).flatten()  # Flatten to 1D array\n",
    "\n",
    "#     # Create lagged features based on the transformed y\n",
    "#     df_lagged = create_lagged_features(pd.DataFrame(y_train_transformed, columns=['incValue']), column='incValue', number_of_lags=lag, seasonal_lags=seasonal)\n",
    "#     df_lagged.dropna(inplace=True)\n",
    "    \n",
    "#     training_cols = [col for col in df_lagged.columns if ('lag_' in col) and ('_helper' not in col)]\n",
    "#     X = df_lagged[training_cols]\n",
    "#     y = df_lagged['incValue']\n",
    "#     print(X)\n",
    "#     print(y)\n",
    "#     # Keep track of configurations and cv scores\n",
    "#     model = MLPRegressor(max_iter=1000, \n",
    "#                         random_state=42, \n",
    "#                         solver='adam', \n",
    "#                         activation=activations, \n",
    "#                         hidden_layer_sizes=hidden_layer_size, \n",
    "#                         alpha=alpha, \n",
    "#                         batch_size=batch_size, \n",
    "#                         learning_rate_init=learning_rate)\n",
    "#     scores = []\n",
    "#     tscv = TimeSeriesSplit(n_splits=5)\n",
    "#     fold = 0\n",
    "#     for train_index, val_index in tscv.split(X):\n",
    "\n",
    "#         X_train_cv, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "#         y_train_cv, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "        \n",
    "#         # Apply scaling within each fold for y\n",
    "#         scaler_y = StandardScaler()\n",
    "#         y_train_cv_scaled = scaler_y.fit_transform(y_train_cv.values.reshape(-1, 1)).flatten()\n",
    "#         y_val_scaled = scaler_y.transform(y_val.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "#         scaler_X = StandardScaler()\n",
    "#         X_train_cv_scaled = scaler_X.fit_transform(X_train_cv)\n",
    "#         X_val_scaled = scaler_X.transform(X_val)\n",
    "        \n",
    "#         # Fit model\n",
    "#         model.fit(X_train_cv_scaled, y_train_cv_scaled)\n",
    "#         # loss_values = model.loss_curve_\n",
    "        \n",
    "#         # Make iterative forecasts (NOTE: train and val splits are numpy arrays, seasonal helper columns necessary for updating of seasonal lag)\n",
    "#         prediction = autoregressive_iterative_forecast(model, X_val_scaled[0], df_lagged['seasonal_lag_52_helper'].iloc[val_index], len(y_val_scaled))\n",
    "#         y_hat_train = autoregressive_iterative_forecast(model, X_train_cv_scaled[0], df_lagged['seasonal_lag_52_helper'].iloc[train_index], len(y_train_cv_scaled))\n",
    "#         prediction = np.array(prediction).flatten()\n",
    "#         y_hat_train = np.array(y_hat_train).flatten()\n",
    "\n",
    "#         # NOTE: UNPLOT FOR PLOTS OF VALIDATION - Plot actual vs predicted values\n",
    "#         plt.figure(figsize=(10, 6))\n",
    "#         plt.plot(range(len(y_train_cv_scaled)), y_train_cv_scaled, label='Training Actual', color='blue')\n",
    "#         plt.plot(range(len(y_train_cv_scaled), len(y_train_cv_scaled) + len(y_val_scaled)), y_val_scaled, label='Validation Actual', color='blue')\n",
    "#         plt.plot(range(len(y_train_cv_scaled), len(y_train_cv_scaled) + len(y_val_scaled)), prediction, label='Validation Predicted', color='red', linestyle='--')\n",
    "#         plt.plot(range(len(y_train_cv_scaled)), y_hat_train, label='Train-Set Prediction', color='orange', linestyle='--')\n",
    "        \n",
    "#         plt.title(f'Lag: {lag}; Fold {fold+1} Predictions vs Actual')\n",
    "#         plt.xlabel('Time')\n",
    "#         plt.ylabel('Scaled Value')\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "        \n",
    "#         # plt.plot(loss_values, label='Training Loss')\n",
    "#         # # If you have validation loss, plot it here\n",
    "#         # # plt.plot(validation_loss_values, label='Validation Loss')\n",
    "\n",
    "#         # plt.title('Learning Curve')\n",
    "#         # plt.xlabel('Epochs')\n",
    "#         # plt.ylabel('Loss')\n",
    "#         # plt.legend()\n",
    "#         # plt.show()\n",
    "        \n",
    "#         rmse = mean_squared_error(y_val_scaled, prediction, squared=False)\n",
    "\n",
    "#         scores.append(rmse)\n",
    "#         fold += 1\n",
    "    \n",
    "#     # Fill in parameters and score for each configuration \n",
    "#     scores_df.loc[i, 'lags'] = lag\n",
    "#     scores_df.loc[i, 'seasonal_lags'] = seasonal\n",
    "#     scores_df.loc[i, 'hidden_layers'] = hidden_layer_size\n",
    "#     scores_df.loc[i, 'alpha'] = alpha\n",
    "#     scores_df.loc[i, 'batch_size'] = batch_size\n",
    "#     scores_df.loc[i, 'activation'] = activations\n",
    "#     scores_df.loc[i, 'learning_rate'] = learning_rate\n",
    "#     scores_df.loc[i, 'RMSE'] = np.mean(scores)\n",
    "#     print(f'{i}/{models_count}: {(i/models_count)*100:.2f}%')\n",
    "#     i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>------------- End Backup</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df['RMSE'] = pd.to_numeric(scores_df['RMSE'])\n",
    "# Best parameters and score\n",
    "best_config_index = scores_df['RMSE'].idxmin()  # This gets the index of the minimum RMSE\n",
    "best_config = scores_df.loc[best_config_index]  # Use the index to access the row\n",
    "best_score = best_config['RMSE']\n",
    "print(f\"Best parameters: {best_config}\")\n",
    "print(f\"Best score (RMSE): {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df.sort_values(by='RMSE').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_config = scores_df.loc[48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_lag = best_config.values[1]\n",
    "best_seasonal_lag = best_config.values[2]\n",
    "best_hidden_layers = best_config.values[3]\n",
    "best_alpha = best_config.values[4]\n",
    "best_batch_size = best_config.values[5]\n",
    "best_activation = best_config.values[6]\n",
    "best_learning_rate = best_config.values[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "data = merged_data.loc[(merged_data['georegion'] == \"region_5\") & (merged_data['date'].apply(lambda x: x.year < 2020))]\n",
    "# Split the data\n",
    "y = data['incValue']\n",
    "\n",
    "# Create lagged features based on the whole y\n",
    "df_lagged = create_lagged_features(pd.DataFrame(y, columns=['incValue']), column='incValue', number_of_lags=best_lag, seasonal_lags=best_seasonal_lag)\n",
    "\n",
    "\n",
    "split = int(len(y) * 0.8)\n",
    "# NOTE: SPLIT BEFORE DROPPING TO AVOID DATA LEAKAGE\n",
    "df_lagged_train = df_lagged.iloc[:split]\n",
    "df_lagged_train = df_lagged_train.dropna()\n",
    "df_lagged_test = df_lagged.iloc[split:]\n",
    "\n",
    "# Extract training columns and output variable from dataframe\n",
    "training_cols = [col for col in df_lagged.columns if ('lag_' in col) and ('helper' not in col)]\n",
    "# Columns required for rolling of seasonal lag in iterative autoregressive forecast\n",
    "helper_cols = [col for col in df_lagged.columns if 'helper' in col]\n",
    "\n",
    "X_train = df_lagged_train[training_cols]\n",
    "X_train_seasonal = df_lagged_train[helper_cols]\n",
    "\n",
    "X_test = df_lagged_test[training_cols]\n",
    "X_test_seasonal = df_lagged_test[helper_cols]\n",
    "\n",
    "y_train = df_lagged_train['incValue']\n",
    "y_test = df_lagged_test['incValue']\n",
    "\n",
    "# Fit Yeo-Johnson Transform\n",
    "# pt = PowerTransformer(method='yeo-johnson')\n",
    "# y_train_transformed = pt.fit_transform(y_train.values.reshape(-1, 1)).flatten()  # Flatten to 1D array\n",
    "\n",
    "\n",
    "# Apply fitted transform to X_train, X_test, y_test\n",
    "# Create copies for scaled variables\n",
    "X_train_scaled, X_test_scaled, y_test_scaled = X_train.copy(), X_test.copy(), y_test.copy()\n",
    "# for col in X_train_scaled.columns:\n",
    "#     X_train_scaled[col] = pt.transform(X_train_scaled[col].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# for col in X_test_scaled.columns:\n",
    "#     X_test_scaled[col] = pt.transform(X_test_scaled[col].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Transform seasonal column helpers and y_test\n",
    "# X_train_seasonal = pt.transform(X_train_seasonal)\n",
    "# X_test_seasonal = pt.transform(X_test_seasonal)\n",
    "# y_test_scaled = pt.transform(y_test_scaled.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "print(f'Test: {X_test_scaled}')\n",
    "print(f'Seasonal lags: {X_test_seasonal}')\n",
    "\n",
    "# Apply StandardScaler\n",
    "scaler_y = StandardScaler()\n",
    "y_train_scaled = scaler_y.fit_transform(np.array(y_train))\n",
    "print(f'\\n\\nBefore scaling:{y_test_scaled}')\n",
    "y_test_scaled = scaler_y.transform(np.array(y_test_scaled))\n",
    "print(f'\\n\\nAfter scaling:{y_test_scaled}')\n",
    "X_train_seasonal = scaler_y.transform(X_train_seasonal)\n",
    "X_test_seasonal = scaler_y.transform(X_test_seasonal)\n",
    "\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train_scaled)\n",
    "X_test_scaled = scaler_X.transform(X_test_scaled)\n",
    "\n",
    "print(f'Test: {pd.DataFrame(X_test_scaled)}')\n",
    "print(f'Seasonal lags: {X_test_seasonal}')\n",
    "\n",
    "# Initialize the final model configuration\n",
    "final_model = MLPRegressor(max_iter=1000, \n",
    "                    random_state=42, \n",
    "                    solver='adam', \n",
    "                    activation=best_activation, \n",
    "                    hidden_layer_sizes=best_hidden_layers, \n",
    "                    alpha=best_alpha, \n",
    "                    batch_size=best_batch_size, \n",
    "                    learning_rate_init=best_learning_rate)\n",
    "\n",
    "# Train final model\n",
    "final_model.fit(X_train_scaled, y_train_scaled) \n",
    "# Forecast for the length of the test set\n",
    "forecasts = autoregressive_iterative_forecast(final_model, X_test_scaled[0], X_test_seasonal,len(y_test_scaled))\n",
    "y_hat_train = autoregressive_iterative_forecast(final_model, X_train_scaled[0], X_train_seasonal, len(y_train))\n",
    "\n",
    "forecasts = scaler_y.inverse_transform(forecasts.reshape(-1, 1))\n",
    "forecasts = pt.inverse_transform(forecasts.reshape(-1, 1))\n",
    "y_hat_train = scaler_y.inverse_transform(y_hat_train.reshape(-1, 1))\n",
    "y_hat_train = pt.inverse_transform(y_hat_train.reshape(-1, 1))\n",
    "\n",
    "# Evaluate the forecasts against the actual y_test values\n",
    "rmse = mean_squared_error(y_test, forecasts, squared=False)\n",
    "\n",
    "print(f\"The RMSE for the forecasts is: {rmse:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_seasonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = data['incValue'].iloc[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = plot.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "# Plot the true values\n",
    "# ax.plot(plot['incValue'])\n",
    "ax.plot(df_lagged['incValue'], label=\"True values\", alpha=0.8)\n",
    "ax.plot(y_test.index, forecasts, label='Predictions', alpha=0.7)\n",
    "ax.plot(y_train.index, y_hat_train, label='Estimation', alpha=0.7)\n",
    "\n",
    "\n",
    "# Add labels and legend\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Incidence\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Exogenous Variables</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
