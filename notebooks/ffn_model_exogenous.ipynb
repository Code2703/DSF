{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from datetime import datetime\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "flu = pd.read_csv('../data/INFLUENZA_sentinella/data.csv')\n",
    "weather = pd.read_csv('../data/weather/reg_weather.csv')\n",
    "google_flu = pd.read_csv('../data/google_search_trend/reg_google_grippe.csv')\n",
    "google_symptoms = pd.read_csv('../data/google_search_trend/reg_google_fieber_husten.csv')\n",
    "pop = pd.read_csv('../data/pop_data_cantons/weekly_imputed_pop_data_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data Consolidation</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Extract relevant data from BAG dataset on weekly flu incidence</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe for regional observations, no differentiation between sex or age\n",
    "flu_reg = flu.query('georegion_type == \"sentinella_region\" and agegroup == \"all\" and sex == \"all\"').copy()\n",
    "\n",
    "# Drop rows for georegion \"unknown\", which only contain NaNs using mask\n",
    "flu_reg = flu_reg[~(flu_reg['georegion'] == 'unknown')]\n",
    "\n",
    "# Select columns required for analysis\n",
    "selected_cols = ['temporal', 'georegion', 'incValue', 'value']\n",
    "flu_reg = flu_reg[selected_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Convert dates and format of Google-Trend data for subsequent merging</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align time-indexes of google data and flu data using created date to iso-calendar week dict\n",
    "with open('date_dict.json', 'r') as f:\n",
    "    date_dict = json.load(f)\n",
    "\n",
    "# Create new column 'Woche' containing iso-calendar weeks for google-trend dates \n",
    "google_flu['Woche'] = google_flu['Woche'].apply(lambda x: date_dict[x]) \n",
    "google_symptoms['Woche'] = google_symptoms['Woche'].apply(lambda x: date_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape google_flu from wide to long to enable merging on date and region \n",
    "google_flu = google_flu.melt(id_vars=['Woche'], var_name='region_query', value_name='search_activity')\n",
    "\n",
    "# Separate region and query information from header into separate rows\n",
    "google_flu['region'] = google_flu['region_query'].apply(lambda x: \"_\".join(x.split('_')[:2]))\n",
    "google_flu['query'] = google_flu['region_query'].apply(lambda x: \"_\".join(x.split('_')[2:]))\n",
    "google_flu.drop(columns='region_query', inplace=True) # Drop superfluous region_query column\n",
    "\n",
    "# Reshape dataframe to get separate columns for each variable\n",
    "google_flu = google_flu.pivot(index=['Woche', 'region'], columns='query', values='search_activity').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Repeat above process for second google dataset containing data on symptom queries\n",
    "# Reshape google_flu from wide to long\n",
    "google_symptoms = google_symptoms.melt(id_vars=['Woche'], var_name='region_query', value_name='search_activity')\n",
    "\n",
    "# Separate region and query information from header into separate rows\n",
    "google_symptoms['region'] = google_symptoms['region_query'].apply(lambda x: \"_\".join(x.split('_')[:2]))\n",
    "google_symptoms['query'] = google_symptoms['region_query'].apply(lambda x: \"_\".join(x.split('_')[2:]))\n",
    "google_symptoms.drop(columns='region_query', inplace=True) # Drop superfluous region_query column\n",
    "\n",
    "# Reshape dataframe to get separate columns for each variable\n",
    "google_symptoms = google_symptoms.pivot(index=['Woche', 'region'], columns='query', values='search_activity').reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Convert date format of weather data for merging</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dates to 'YYYY-Www' ISO week format\n",
    "dates = weather.date.values\n",
    "iso_week_dates = [datetime.strptime(date, '%Y-%m-%d').isocalendar()[:2] for date in dates]\n",
    "iso_week_dates = [f'{year}-W{week:02d}' for year, week in iso_week_dates]\n",
    "weather['date'] = iso_week_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Merge datasets on date and region</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.merge(flu_reg, weather, how='left', left_on=['temporal', 'georegion'], right_on=['date', 'region']).sort_values(by=['georegion', 'temporal'])\n",
    "merged_google = pd.merge(google_flu, google_symptoms, how='inner', on=['region', 'Woche'])\n",
    "merged_data = pd.merge(merged_data, merged_google, how='left', left_on=['georegion', 'temporal'], right_on=['region', 'Woche'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.drop(columns=['region_x', 'region_y', 'date', 'Woche'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Convert ISO calendar-weeks to Gregorian calendar (format 'YYYY-MM-DD')</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Convert ISO calendar-weeks to gregorian dates\n",
    "# Functions based on answer by Ben James: <https://stackoverflow.com/questions/304256/whats-the-best-way-to-find-the-inverse-of-datetime-isocalendar>\n",
    "def iso_year_start(iso_year):\n",
    "    \"The gregorian calendar date of the first day of the given ISO year\"\n",
    "    fourth_jan = datetime.date(iso_year, 1, 4)\n",
    "    delta = datetime.timedelta(fourth_jan.isoweekday()-1)\n",
    "    return fourth_jan - delta \n",
    "\n",
    "def iso_to_gregorian(iso_year, iso_week, iso_day):\n",
    "    \"Gregorian calendar date for the given ISO year, week and day\"\n",
    "    year_start = iso_year_start(iso_year)\n",
    "    return year_start + datetime.timedelta(days=iso_day-1, weeks=iso_week-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract week number and year from date column in ISO calendar week \n",
    "week_pattern = r'W(\\d{1,2})' # RegEx pattern to extract week nr without trailing zero\n",
    "merged_data['week_number'] = merged_data['temporal'].str.extract(week_pattern).astype(int)\n",
    "merged_data['year'] = merged_data['temporal'].apply(lambda x: x.split('-')[0])\n",
    "merged_data['year'] = pd.to_numeric(merged_data['year']) # Convert from string to numeric\n",
    "\n",
    "# Convert from iso-calendar week to gregorian dates (format: YYYY-MM-DD)\n",
    "merged_data['date'] = list(map(lambda year, week: iso_to_gregorian(year, week, 4), merged_data['year'], merged_data['week_number']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect resulting dataframe\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data exploration</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display descriptive statistics\n",
    "merged_data['incValue'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display missing values in reported flu incidence across regions\n",
    "merged_data[merged_data['incValue'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values in March 2020 linearly\n",
    "merged_data['incValue'].interpolate(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Inspect incidence of consultations for influenza-like-diseases over time</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(6, figsize=(12, 12))\n",
    "# for i in range(1, 7):\n",
    "#     ax[i-1].plot(merged_data.loc[merged_data['georegion'] == f\"region_{i}\", 'incValue'])\n",
    "#     ax[i-1].plot(merged_data.loc[merged_data['georegion'] == f\"region_{i}\", 'Grippe'])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Dictionary for cantons within each region\n",
    "region_to_ct = {'region_1': ['Genf', 'Neuenburg', 'Waadt', 'Wallis'], \n",
    "           'region_2': ['Bern', 'Freiburg', 'Jura'], \n",
    "           'region_3': ['Aargau', 'Basel-Landschaft', 'Basel-Stadt', 'Solothurn'], \n",
    "           'region_4': ['Luzern', 'Nidwalden', 'Obwalden', 'Schwyz', 'Uri', 'Zug'], \n",
    "           'region_5': ['Appenzell_Innerrhoden', 'Appenzell_Ausserrhoden', 'Glarus', 'Sankt_Gallen', 'Schaffhausen', 'Thurgau', 'Zürich'], \n",
    "           'region_6': ['Graubünden', 'Tessin']}\n",
    "\n",
    "# Assuming merged_data is your DataFrame and it's already been imported.\n",
    "plt.style.use('ggplot')\n",
    "fig, ax = plt.subplots(6, figsize=(10, 12))\n",
    "\n",
    "for i in range(1, 7):\n",
    "    # Filter the data for the current region\n",
    "    region_data = merged_data[merged_data['georegion'] == f\"region_{i}\"]\n",
    "    \n",
    "    # Find the maximum values for 'incValue' and 'Grippe' in the current region\n",
    "    max_incValue_region = region_data['incValue'].max()\n",
    "    max_Grippe_region = region_data['Grippe'].max()\n",
    "    \n",
    "    # Determine the scale factors\n",
    "    scale_factor_incValue = 1.0 / max_incValue_region if max_incValue_region != 0 else 1\n",
    "    scale_factor_Grippe = 1.0 / max_Grippe_region if max_Grippe_region != 0 else 1\n",
    "    \n",
    "    # Apply the scale factors to normalize the amplitude\n",
    "    normalized_incValue = region_data['incValue'] * scale_factor_incValue\n",
    "    normalized_Grippe = region_data['Grippe'] * scale_factor_Grippe\n",
    "    \n",
    "    # Plot the normalized data\n",
    "    ax[i-1].plot(normalized_incValue, alpha=.8)\n",
    "    ax[i-1].plot(normalized_Grippe, color='blue', alpha=.8)\n",
    "    ax[i-1].set_title(f'Region {i}: {region_to_ct[f\"region_{i}\"]}', fontsize=10)\n",
    "\n",
    "plt.suptitle('Normalized incValue and Google Search Activity per Region for the time frame between 2013 and late 2023')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Modelling</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the above plot demonstrates, google search search results for the term 'Grippe' follow the incidence of consultations for influenza-like-diseases quite closely. However, in the regions with less population, such as region 4 and 6, the quality of the data suffers dramatically. The following model attempts to refine the predictions from the purely autoregressive neural network by also considerung varying ranges of lags for the search term 'Grippe' in region 5. \n",
    "\n",
    "In order to do so, the following functions are required:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* create_lagged_features: Outputs a dataframe with columns at a specified range of lags given a dictionary specifying the column name and the number of lags to create for that column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def create_lagged_features(df, column_lags, seasonal_lags=None):\n",
    "    \"\"\"\n",
    "    Create lagged features for multiple columns in a DataFrame, with a specified number of lags for each column.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The original DataFrame.\n",
    "    columns_with_lags (dict): A dictionary mapping column names to the number of lags. e.g., {'column1': 3, 'column2': 5}\n",
    "    seasonal_lags (list of int, optional): Additional seasonal lags to include.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: A DataFrame containing the original columns and their lagged features.\n",
    "    \"\"\"\n",
    "\n",
    "    df_lagged = df.copy()\n",
    "    seasonal_lags = seasonal_lags or []\n",
    "\n",
    "    for column, num_lags in column_lags.items():\n",
    "        # Create specified number of lagged features for each column\n",
    "        for lag in range(1, num_lags + 1):\n",
    "            df_lagged[f'{column}_lag_{lag}'] = df_lagged[column].shift(lag)\n",
    "\n",
    "        # Create seasonal lagged features if specified\n",
    "        for seasonal_lag in seasonal_lags:\n",
    "            df_lagged[f'{column}_seasonal_lag_{seasonal_lag}_helper'] = df_lagged[column].shift(seasonal_lag-1)\n",
    "            df_lagged[f'{column}_seasonal_lag_{seasonal_lag}'] = df_lagged[column].shift(seasonal_lag)\n",
    "\n",
    "    return df_lagged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* iterative_forecast: Perform iterative one-step-ahead forecasting using a pretrained model for a specified number of steps given initial input of lag observations and an array of forecasts for the exogenous variable for the length of the forecast (created separately using function \"create_exogenous_input\" below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_forecast(model, initial_input, seasonal_input,  n_steps, exogenous_forecasts=None):\n",
    "    \"\"\"\n",
    "    Perform iterative forecasting using a pretrained model with optional exogenous variable forecasts.\n",
    "\n",
    "    Args:\n",
    "        model: Trained autoregressive model (e.g., MLPRegressor).\n",
    "        initial_input: The initial input features for the target variable (e.g., the last observation from the training set).\n",
    "        seasonal_input: Seasonal input features for the target variable.\n",
    "        exogenous_forecasts: (Optional) List or array containing the forecasted values for the exogenous variable(s) for each future time step.\n",
    "        n_steps: Number of future time steps to forecast.\n",
    "\n",
    "    Returns:\n",
    "        A list of forecasts, one for each future time step.\n",
    "    \"\"\"\n",
    "    # Initiate empty list for forecasts\n",
    "    forecasts = []\n",
    "\n",
    "    # Transform inputs into np.arrays\n",
    "    current_input = np.array(initial_input)\n",
    "    seasonal_input = np.array(seasonal_input)\n",
    "\n",
    "    # Check if exogenous forecasts are provided\n",
    "    if exogenous_forecasts is None:\n",
    "        exogenous_forecasts = np.empty((n_steps, 0))  # Empty array with 0 columns\n",
    "\n",
    "    for i in range(n_steps):\n",
    "        # Prepare the input array for prediction\n",
    "        combined_input = np.concatenate([\n",
    "            current_input,\n",
    "            exogenous_forecasts[i] if exogenous_forecasts.shape[1] > 0 else []  # Add the forecasted exogenous values if provided\n",
    "        ])\n",
    "        \n",
    "        # Predict the next step based on the combined input\n",
    "        next_step_pred = model.predict(combined_input.reshape(1, -1))[0]\n",
    "        forecasts.append(next_step_pred)\n",
    "\n",
    "        # Update the autoregressive lags based on the forecast (next_step_pred becomes most recent lag)\n",
    "        current_input = np.roll(current_input, 1)\n",
    "        current_input[0] = next_step_pred\n",
    "        \n",
    "        # Update the seasonal lag based on true values for the first year and the forecasted values from then on\n",
    "        if i < 52:\n",
    "            current_input[-1] = seasonal_input[i]\n",
    "        else:\n",
    "            current_input[-1] = forecasts[i - 52]\n",
    "\n",
    "    return np.array(forecasts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* create_exogenous_input: Based on a separate forecasting model (in this case the same architecture for the google data) predict the required timeframe ahead for the exogenous variable and output an array containing forecasts for the lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_exogenous_input(model, initial_input, seasonal_input, n_steps):\n",
    "    \"\"\"\n",
    "    Create a 2D array for exogenous variable inputs for each forecast step.\n",
    "\n",
    "    Args:\n",
    "        model: Model for exogenous variable to predict one week ahead.\n",
    "        initial_input (array-like): The initial lag values for the exogenous variable.\n",
    "        seasonal_input (array-like): Array of the values one week before the seasonal column for updating (essentially the lag_51 column, named 'seasonal_helper' in this notebook).\n",
    "        n_steps (int): Number of future time steps to forecast.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A 2D array where each row represents all the lags and the seasonal lag for the exogenous variable for each week.\n",
    "    \"\"\"\n",
    "    num_lags = len(initial_input)\n",
    "    \n",
    "    initial_input = np.array(initial_input)\n",
    "    seasonal_input = np.array(seasonal_input)\n",
    "    \n",
    "    output_array = np.zeros((n_steps, num_lags))\n",
    "\n",
    "    # Set initial lags and seasonal lag\n",
    "    output_array[0, :] = initial_input\n",
    "\n",
    "    forecasts = []\n",
    "    for i in range(1, n_steps):\n",
    "        # Predict the next step\n",
    "        next_step_pred = model.predict(output_array[i-1,:].reshape(1, -1))[0]\n",
    "        forecasts.append(next_step_pred)\n",
    "\n",
    "        # Update lags: shift left and add new forecast\n",
    "        output_array[i, :-1] = np.roll(output_array[i-1, :-1], 1) # Roll the consecutive autoregressive lags from one row before to shift weeks\n",
    "        output_array[i, 0] = next_step_pred  # Insert most recent forecast in first position\n",
    "\n",
    "        # Update seasonal lag\n",
    "        if i < 52:\n",
    "            output_array[i, -1] = seasonal_input[i-1]\n",
    "        else:\n",
    "            output_array[i, -1] = output_array[i-52, 0] # Fill in using the forecast from 52 weeks ago\n",
    "\n",
    "    return output_array # Don't return the first column where value are all known to avoid shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Model Tuning</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following model, we're attempting to introduce Google search trend data for the query 'Grippe' (the German word for flu) for greater predictive power.\n",
    "\n",
    "The model encapsulates a second model which forecasts the exogenous variable for the demanded timeframe. The configuration of the second model has been tuned separately and is also a feedforward neural network in this case (see notebook \"ffn_exogenous.ipynb\"). Based on the predicted exogenous features and its own initial autoregressive lags, the model then forecasts the incidence in an iterative procedure, predicting one-step ahead and using this as the most recent lag for the next step. An alternative to such iterative forecasting would be to directly model multiple steps ahead. Such methods couldn't be considered due to time constraints but should be explored in continued research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "data = merged_data.loc[(merged_data['georegion'] == \"region_5\") & (merged_data['date'].apply(lambda x: x.year < 2020))]\n",
    "# Split the data\n",
    "y = data['incValue']\n",
    "split = int(len(y) * 0.8)\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "i = 0\n",
    "scores_df = pd.DataFrame(columns=['RMSE', 'lags', 'seasonal_lags', 'hidden_layers', 'alpha', 'batch_size', 'activation', 'learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress convergence warnings\n",
    "# warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "\n",
    "# Define parameter configurations to assess\n",
    "lags = 52 # Autoregressive lags to consider\n",
    "hidden_layer_sizes = [(16, 16), (16, 16, 16)]\n",
    "alphas = np.linspace(0.01, 0.3, num=100) # Regularization parameter\n",
    "batch_size = 32\n",
    "learning_rates = np.logspace(-3, -4, 100)\n",
    "activations = ['relu']\n",
    "seasonal = [52]\n",
    "exog_lags = [21]\n",
    "\n",
    "models_count = (lags-44) * 18 * len(hidden_layer_sizes) * len(alphas) * len(learning_rates) * len(activations)\n",
    "\n",
    "# Filter the DataFrame\n",
    "data = merged_data.loc[\n",
    "    (merged_data['georegion'] == \"region_5\") & \n",
    "    (merged_data['date'].apply(lambda x: (x.year < 2020))) & \n",
    "    (merged_data['date'].apply(lambda x: (x > datetime.datetime.strptime('2013-01-03', '%Y-%m-%d').date())))\n",
    "]\n",
    "# Split the data\n",
    "y = data['incValue']\n",
    "grippe = data['Grippe']\n",
    "split = int(len(y) * 0.8)\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "grippe_train, grippe_test = grippe[:split], grippe[split:]\n",
    "\n",
    "i = 0\n",
    "scores_df = pd.DataFrame(columns=['RMSE', 'lags', 'seasonal_lags', 'hidden_layers', 'alpha', 'batch_size', 'activation', 'learning_rate', 'exog_lags', 'grippe_lags'])\n",
    "\n",
    "random.seed(42)\n",
    "iterations = 500\n",
    "\n",
    "# Grid search hyperparameter configurations\n",
    "\n",
    "for _ in range(iterations):\n",
    "    # Randomly select hyperparameters\n",
    "    lag = random.randint(0, lags-20)\n",
    "    activation = random.choice(activations)\n",
    "    learning_rate = random.choice(learning_rates)\n",
    "    alpha = random.choice(alphas)\n",
    "    hidden_layer_size = random.choice(hidden_layer_sizes)\n",
    "    exog_lag = random.choice(exog_lags)\n",
    "    grippe_lag = random.randint(1, exog_lag)\n",
    "    # Keep track of configurations and cv scores\n",
    "    model = MLPRegressor(max_iter=1000, \n",
    "                        random_state=42, \n",
    "                        solver='adam', \n",
    "                        activation=activation, \n",
    "                        hidden_layer_sizes=hidden_layer_size, \n",
    "                        alpha=alpha, \n",
    "                        batch_size=batch_size, \n",
    "                        learning_rate_init=learning_rate,\n",
    "                        warm_start=False, \n",
    "                        early_stopping=True)\n",
    "\n",
    "    grippe_model = MLPRegressor(max_iter=2000, \n",
    "                        random_state=42, \n",
    "                        solver='adam', \n",
    "                        activation='relu', \n",
    "                        hidden_layer_sizes=(16, 16, 16), \n",
    "                        alpha=0.1343, \n",
    "                        batch_size=32, \n",
    "                        learning_rate_init=0.000413)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    # Define number of lags to create in the training set (exogenous variables config cross-validated separately)\n",
    "    column_lags = {\n",
    "        'incValue': lag,\n",
    "        'Grippe': exog_lag\n",
    "        }\n",
    "\n",
    "    # Create lagged features based on the whole available training data\n",
    "    df_lagged = create_lagged_features(pd.DataFrame({'incValue': y_train, 'Grippe': grippe_train}), column_lags=column_lags, seasonal_lags=[52])\n",
    "    df_lagged.dropna(inplace=True)\n",
    "    \n",
    "    # Assign columns for preprocessing and training of autoregressive component\n",
    "    flu_training_cols = [col for col in df_lagged.columns if ('incValue' in col) and ('lag' in col) and ('_helper' not in col)] # Select cols to include as features\n",
    "    flu_seasonal_col = [col for col in df_lagged.columns if ('incValue' in col) and ('_helper' in col)] # Select cols to include as features\n",
    "    X = df_lagged[flu_training_cols]\n",
    "    X_seasonal = df_lagged[flu_seasonal_col]\n",
    "    y = df_lagged['incValue']\n",
    "\n",
    "    # Assign columns for preprocessing and training of exogenous component\n",
    "    grippe_training_cols = [col for col in df_lagged.columns if ('Grippe' in col) and ('lag' in col) and ('_helper' not in col)] # Select cols to include as features\n",
    "    grippe_seasonal_col = [col for col in df_lagged.columns if ('Grippe' in col) and ('_helper' in col)] # Select cols to include as features\n",
    "    grippe = df_lagged[grippe_training_cols]\n",
    "    grippe_seasonal = df_lagged[grippe_seasonal_col]\n",
    "    grippe_y = df_lagged['Grippe']\n",
    "    \n",
    "    train_index = range(0, len(y) - 52)\n",
    "    val_index = range(len(y) - 52, len(y))\n",
    "\n",
    "        \n",
    "    ##################################  TRANSFORMATION AND SCALING OF AUTOREGRESSIVE LAGS  #######################################################\n",
    "    # \n",
    "    y_train_cv, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    X_train_cv, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    X_seasonal_train, X_seasonal_val = X_seasonal.iloc[train_index], X_seasonal.iloc[val_index]\n",
    "\n",
    "    # Take the first row of X_train_cv (the oldest lags)\n",
    "    oldest_lags = X_train_cv.iloc[0, 1:].values.reshape(1, -1)\n",
    "\n",
    "    # Concatenate y_train_cv with the oldest lags\n",
    "    combined_data = np.vstack((y_train_cv.values.reshape(-1, 1), oldest_lags.T))\n",
    "\n",
    "    # Fit the PowerTransformer and StandardScaler on the available lags in the training data (incl. lags in first row of lag df_train)\n",
    "    pt = PowerTransformer(method='yeo-johnson')\n",
    "    stdscaler = StandardScaler()\n",
    "    combined_data_transformed = pt.fit_transform(combined_data)\n",
    "    stdscaler.fit(combined_data_transformed)\n",
    "    \n",
    "    # Apply Transform to the entire y_train_cv\n",
    "    y_train_cv_transformed = pt.transform(y_train_cv.values.reshape(-1, 1)).flatten()\n",
    "    y_val_transformed = pt.transform(y_val.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Apply the PowerTransformer to each lagged feature in X_train_cv and X_val\n",
    "    X_train_cv_transformed = X_train_cv.apply(lambda column: pt.transform(column.values.reshape(-1, 1)).flatten())\n",
    "    X_val_transformed = X_val.apply(lambda column: pt.transform(column.values.reshape(-1, 1)).flatten())\n",
    "    X_seasonal_train_trans = pt.transform(X_seasonal_train.values.reshape(-1, 1)).flatten()\n",
    "    X_seasonal_val_trans = pt.transform(X_seasonal_val.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "    \n",
    "    # Apply StandardScaler()\n",
    "    y_train_cv_scaled = stdscaler.transform(y_train_cv_transformed.reshape(-1, 1)).flatten()\n",
    "    y_val_scaled = stdscaler.transform(y_val_transformed.reshape(-1, 1)).flatten()\n",
    "    X_train_cv_scaled = X_train_cv_transformed.apply(lambda column: stdscaler.transform(column.values.reshape(-1, 1)).flatten())\n",
    "    X_val_scaled = X_val_transformed.apply(lambda column: stdscaler.transform(column.values.reshape(-1, 1)).flatten())\n",
    "    X_seasonal_train_scaled = stdscaler.transform(X_seasonal_train_trans.reshape(-1, 1)).flatten()\n",
    "    X_seasonal_val_scaled = stdscaler.transform(X_seasonal_val_trans.reshape(-1, 1)).flatten()\n",
    "\n",
    "    #########################################  SCALE GOOGLE DATA  ##################################################\n",
    "    # Scale Google Data\n",
    "    grippe_y_train_cv, grippe_y_val = grippe_y.iloc[train_index], grippe_y.iloc[val_index]\n",
    "    grippe_train_cv, grippe_val = grippe.iloc[train_index], grippe.iloc[val_index]\n",
    "    grippe_seasonal_train, grippe_seasonal_val = grippe_seasonal.iloc[train_index], grippe_seasonal.iloc[val_index]\n",
    "    # Take the first row of X_train_cv (the oldest lags)\n",
    "    oldest_grippe_lags = grippe_train_cv.iloc[0, 1:].values.reshape(1, -1)\n",
    "    # Concatenate grippe_y_train_cv with the oldest lags\n",
    "    grippe_combined_data = np.vstack((grippe_y_train_cv.values.reshape(-1, 1), oldest_grippe_lags.T))\n",
    "    \n",
    "    # Fit the PowerTransformer and StandardScaler on the available lags in the training data (incl. lags in first row of lag df_train)\n",
    "    grippe_pt = PowerTransformer(method='yeo-johnson')\n",
    "    grippe_stdscaler = StandardScaler()\n",
    "    grippe_combined_data_transformed = grippe_pt.fit_transform(grippe_combined_data)\n",
    "    grippe_stdscaler.fit(grippe_combined_data_transformed)\n",
    "\n",
    "    # Apply Transform to the entire y_train_cv\n",
    "    grippe_y_train_cv_transformed = pt.transform(grippe_y_train_cv.values.reshape(-1, 1)).flatten()\n",
    "    grippe_y_val_transformed = pt.transform(grippe_y_val.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Apply the PowerTransformer to each lagged feature in X_train_cv and X_val\n",
    "    grippe_train_cv_transformed = grippe_train_cv.apply(lambda column: grippe_pt.transform(column.values.reshape(-1, 1)).flatten())\n",
    "    grippe_val_transformed = grippe_val.apply(lambda column: grippe_pt.transform(column.values.reshape(-1, 1)).flatten())\n",
    "    grippe_seasonal_train_trans = grippe_pt.transform(grippe_seasonal_train.values.reshape(-1, 1)).flatten()\n",
    "    grippe_seasonal_val_trans = grippe_pt.transform(grippe_seasonal_val.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Apply StandardScaler()\n",
    "    grippe_y_train_cv_scaled = grippe_stdscaler.transform(grippe_y_train_cv_transformed.reshape(-1, 1)).flatten()\n",
    "    grippe_y_val_scaled = grippe_stdscaler.transform(grippe_y_val_transformed.reshape(-1, 1)).flatten()\n",
    "    grippe_train_cv_scaled = grippe_train_cv_transformed.apply(lambda column: grippe_stdscaler.transform(column.values.reshape(-1, 1)).flatten())\n",
    "    grippe_val_scaled = grippe_val_transformed.apply(lambda column: grippe_stdscaler.transform(column.values.reshape(-1, 1)).flatten())\n",
    "    grippe_seasonal_train_scaled = grippe_stdscaler.transform(grippe_seasonal_train_trans.reshape(-1, 1)).flatten()\n",
    "    grippe_seasonal_val_scaled = grippe_stdscaler.transform(grippe_seasonal_val_trans.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "    #######################################  FORECASTING GOOGLE DATA   ################################################################\n",
    "    grippe_model.fit(grippe_train_cv_scaled.values, grippe_y_train_cv_scaled)\n",
    "\n",
    "    grippe_input = create_exogenous_input(model=grippe_model, \n",
    "                                            initial_input=grippe_val_scaled.iloc[0], \n",
    "                                            seasonal_input=grippe_seasonal_val_scaled, \n",
    "                                            n_steps=len(grippe_y_val_scaled))\n",
    "    \n",
    "    grippe_input_on_test = create_exogenous_input(model=grippe_model, \n",
    "                                                    initial_input=grippe_train_cv_scaled.iloc[0], \n",
    "                                                    seasonal_input=grippe_seasonal_train_scaled, \n",
    "                                                    n_steps=len(grippe_y_train_cv_scaled))\n",
    "\n",
    "    #########################################  PLOT VALIDATION LOSSES IF REQUIRED  ###########################################################\n",
    "    # NOTE: PLOT VALIDATION AND TRAINING LOSSES - Adjust max_iter to 1 and set warm_start = True to enable\n",
    "\n",
    "    # training_losses = []\n",
    "    # validation_losses = []\n",
    "\n",
    "    # for epoch in range(1000):  # Adjust the number of epochs as needed\n",
    "    #     model.fit(X_train_cv_scaled.values, y_train_cv_scaled)\n",
    "\n",
    "    #     # Store training loss from the last iteration\n",
    "    #     training_losses.append(model.loss_curve_[-1])\n",
    "\n",
    "    #     # Compute and store validation loss\n",
    "    #     val_predictions = model.predict(X_val_scaled.values)\n",
    "    #     val_loss = mean_squared_error(y_val_scaled, val_predictions)\n",
    "    #     validation_losses.append(val_loss)\n",
    "    \n",
    "    # if fold == 2:\n",
    "    #     plt.plot(training_losses, label='Training Loss')\n",
    "    #     # If you have validation loss, plot it here\n",
    "    #     plt.plot(validation_losses, label='Validation Loss')\n",
    "\n",
    "    #     plt.title('Learning Curve')\n",
    "    #     plt.xlabel('Epochs')\n",
    "    #     plt.ylabel('Loss')\n",
    "    #     plt.title(f'Lags: {lag}, Learning-rate: {learning_rate}, alpha: {alpha}, hidden layers: {hidden_layer_size}')\n",
    "    #     plt.legend()\n",
    "    #     plt.show()\n",
    "\n",
    "\n",
    "    ########################################################  FIT MODEL AND MAKE INCVALUE FORECASTS  ####################################################################\n",
    "    \n",
    "    # Fit model with varying numbers of lags of the exogenous variable\n",
    "    model.fit(np.concatenate(\n",
    "        (\n",
    "            X_train_cv_scaled.values, \n",
    "            grippe_train_cv_scaled.values[:, :grippe_lag],  \n",
    "            grippe_train_cv_scaled.values[:, -1:]\n",
    "        ), \n",
    "        axis=1), \n",
    "        y_train_cv_scaled\n",
    "        )\n",
    "    \n",
    "    # Make iterative forecasts (NOTE: train and val splits are numpy arrays, seasonal helper columns necessary for updating of seasonal lag)\n",
    "    prediction = iterative_forecast(model=model, \n",
    "                                    initial_input=X_val_scaled.iloc[0], \n",
    "                                    seasonal_input=X_seasonal_val_scaled, \n",
    "                                    exogenous_forecasts=np.concatenate((grippe_input[:, :grippe_lag], grippe_input[:, -1:]), axis=1), \n",
    "                                    n_steps=len(y_val_scaled))\n",
    "    \n",
    "    # y_hat_train = iterative_forecast(model=model, initial_input=X_train_cv_scaled.iloc[0], seasonal_input=X_seasonal_train_scaled, exogenous_forecasts=grippe_input, n_steps=len(y_train_cv_scaled))\n",
    "    prediction = np.array(prediction).flatten()\n",
    "    # y_hat_train = np.array(y_hat_train).flatten()\n",
    "\n",
    "    #######################\n",
    "    # NOTE: UNCOMMENT FOR PLOTS OF VALIDATION - Plot actual vs predicted values\n",
    "    # plt.figure(figsize=(10, 6))\n",
    "    # plt.plot(range(len(y_train_cv_scaled)), y_train_cv_scaled, label='Training Actual', color='blue')\n",
    "    # plt.plot(range(len(y_train_cv_scaled), len(y_train_cv_scaled) + len(y_val_scaled)), y_val_scaled, label='Validation Actual', color='blue')\n",
    "    # plt.plot(range(len(y_train_cv_scaled), len(y_train_cv_scaled) + len(y_val_scaled)), prediction, label='Validation Predicted', color='red', linestyle='--')\n",
    "    # plt.plot(range(len(y_train_cv_scaled)), y_hat_train, label='Train-Set Prediction', color='orange', linestyle='--')\n",
    "    \n",
    "    # plt.title(f'Lag: {lag}; Predictions vs Actual')\n",
    "    # plt.xlabel('Time')\n",
    "    # plt.ylabel('Scaled Value')\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "    ########################\n",
    "\n",
    "    rmse = mean_squared_error(y_val_scaled, prediction, squared=False)\n",
    "    scores.append(rmse)\n",
    "    \n",
    "    # Fill in parameters and score for each configuration \n",
    "    scores_df.loc[i, 'lags'] = lag\n",
    "    scores_df.loc[i, 'seasonal_lags'] = seasonal\n",
    "    scores_df.at[i, 'hidden_layers'] = hidden_layer_size\n",
    "    scores_df.loc[i, 'alpha'] = alpha\n",
    "    scores_df.loc[i, 'batch_size'] = batch_size\n",
    "    scores_df.loc[i, 'activation'] = activation\n",
    "    scores_df.loc[i, 'learning_rate'] = learning_rate\n",
    "    scores_df.loc[i, 'exogenous_lags'] = exog_lag\n",
    "    scores_df.loc[i, 'grippe_lag'] = grippe_lag\n",
    "    scores_df.loc[i, 'RMSE'] = np.mean(scores)\n",
    "    print(f'{i}/{iterations}: {(i/iterations)*100:.2f}%')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df['RMSE'] = pd.to_numeric(scores_df['RMSE'])\n",
    "# Best parameters and score\n",
    "best_config_index = scores_df['RMSE'].idxmin()  # This gets the index of the minimum RMSE\n",
    "best_config = scores_df.loc[best_config_index]  # Use the index to access the row\n",
    "best_score = best_config['RMSE']\n",
    "print(f\"Best parameters: {best_config}\")\n",
    "print(f\"Best score (RMSE): {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_config.values[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df.sort_values(by='RMSE').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Predict on Test Set</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_nr = 1\n",
    "\n",
    "for index_nr in scores_df.sort_values(by='RMSE').head(10).index:\n",
    "    best_config = scores_df.loc[index_nr]\n",
    "\n",
    "    best_lag = best_config.values[1]\n",
    "    best_seasonal_lag = best_config.values[2]\n",
    "    best_hidden_layers = best_config.values[3]\n",
    "    best_alpha = best_config.values[4]\n",
    "    best_batch_size = best_config.values[5]\n",
    "    best_activation = best_config.values[6]\n",
    "    best_learning_rate = best_config.values[7]\n",
    "    best_exogenous_lag = int(best_config.values[10])\n",
    "    best_grippe_lag = int(best_config.values[11])\n",
    "\n",
    "    # Filter the DataFrame\n",
    "    data = merged_data.loc[\n",
    "        (merged_data['georegion'] == \"region_5\") & \n",
    "        (merged_data['date'].apply(lambda x: (x.year < 2020))) & \n",
    "        (merged_data['date'].apply(lambda x: (x > datetime.datetime.strptime('2013-01-03', '%Y-%m-%d').date())))\n",
    "    ]\n",
    "    # Split the data\n",
    "    y = data['incValue']\n",
    "    grippe_y = data['Grippe']\n",
    "\n",
    "    column_lags = {\n",
    "                    'incValue': best_lag,\n",
    "                    'Grippe': int(best_grippe_lag)\n",
    "                    }\n",
    "        \n",
    "    # Create lagged features based on the whole available training data\n",
    "    df_lagged = create_lagged_features(pd.DataFrame({'incValue': y, 'Grippe': grippe_y}), column_lags=column_lags, seasonal_lags=[52])\n",
    "\n",
    "    split = int(len(y) * 0.8)\n",
    "    # NOTE: SPLIT BEFORE DROPPING TO AVOID DATA LEAKAGE\n",
    "    df_lagged_train = df_lagged.iloc[:split]\n",
    "    df_lagged_train = df_lagged_train.dropna()\n",
    "    df_lagged_test = df_lagged.iloc[split:]\n",
    "\n",
    "    grippe_training_cols = [col for col in df_lagged.columns if ('Grippe' in col) and ('lag' in col) and ('_helper' not in col)] # Select cols to include as features\n",
    "    grippe_train = df_lagged_train[grippe_training_cols]\n",
    "    grippe_y_train = df_lagged_train['Grippe']\n",
    "    grippe_test = df_lagged_test[grippe_training_cols]\n",
    "    grippe_y_test = df_lagged_test['Grippe']\n",
    "\n",
    "    # Columns required for rolling of seasonal lag in iterative autoregressive forecast\n",
    "    grippe_seasonal_col = [col for col in df_lagged.columns if ('Grippe' in col) and ('_helper' in col)] # Select cols to include as features\n",
    "    grippe_train_seasonal = df_lagged_train[grippe_seasonal_col]\n",
    "    grippe_test_seasonal = df_lagged_test[grippe_seasonal_col]\n",
    "\n",
    "    # Take the first row of X_train_cv (the oldest lags)\n",
    "    oldest_grippe_lags = grippe_train.iloc[0, 1:].values.reshape(1, -1)\n",
    "    # Concatenate grippe_y_train_cv with the oldest lags\n",
    "    grippe_combined_data = np.vstack((grippe_y_train.values.reshape(-1, 1), oldest_grippe_lags.T))\n",
    "\n",
    "    # Fit the PowerTransformer and StandardScaler on the available lags in the training data (incl. lags in first row of lag df_train)\n",
    "    grippe_pt = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "    grippe_stdscaler = StandardScaler()\n",
    "    grippe_combined_data_transformed = grippe_pt.fit_transform(grippe_combined_data)\n",
    "    grippe_stdscaler.fit(grippe_combined_data_transformed)\n",
    "\n",
    "    # Apply Transform to the entire y_train\n",
    "    grippe_y_train_transformed = grippe_pt.transform(grippe_y_train.values.reshape(-1, 1)).flatten()\n",
    "    grippe_y_test_transformed = grippe_pt.transform(grippe_y_test.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Apply the PowerTransformer to each lagged feature in X_train and X_test\n",
    "    grippe_train_transformed = grippe_train.apply(lambda column: grippe_pt.transform(column.values.reshape(-1, 1)).flatten())\n",
    "    grippe_test_transformed = grippe_test.apply(lambda column: grippe_pt.transform(column.values.reshape(-1, 1)).flatten())\n",
    "    grippe_train_seasonal_trans = grippe_pt.transform(grippe_train_seasonal.values.reshape(-1, 1)).flatten()\n",
    "    grippe_test_seasonal_trans = grippe_pt.transform(grippe_test_seasonal.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Apply StandardScaler()\n",
    "    grippe_y_train_scaled = grippe_stdscaler.transform(grippe_y_train_transformed.reshape(-1, 1)).flatten()\n",
    "    grippe_y_test_scaled = grippe_stdscaler.transform(grippe_y_test_transformed.reshape(-1, 1)).flatten()\n",
    "    grippe_train_scaled = grippe_train_transformed.apply(lambda column: grippe_stdscaler.transform(column.values.reshape(-1, 1)).flatten())\n",
    "    grippe_test_scaled = grippe_test_transformed.apply(lambda column: grippe_stdscaler.transform(column.values.reshape(-1, 1)).flatten())\n",
    "    grippe_train_seasonal_scaled = grippe_stdscaler.transform(grippe_train_seasonal_trans.reshape(-1, 1)).flatten()\n",
    "    grippe_test_seasonal_scaled = grippe_stdscaler.transform(grippe_test_seasonal_trans.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Initialize configuration of MLPRegressor based on CV results from separate training\n",
    "    grippe_model = MLPRegressor(max_iter=2000, \n",
    "                                random_state=42, \n",
    "                                solver='adam', \n",
    "                                activation='relu', \n",
    "                                hidden_layer_sizes=(16, 16), \n",
    "                                alpha=0.2, \n",
    "                                batch_size=32, \n",
    "                                learning_rate_init=0.0001)\n",
    "\n",
    "    grippe_model.fit(grippe_train_scaled.values, grippe_y_train_scaled)\n",
    "\n",
    "    grippe_input = create_exogenous_input(model=grippe_model, initial_input=grippe_test_scaled.iloc[0], seasonal_input=grippe_test_seasonal_scaled, n_steps=len(grippe_y_test_scaled))\n",
    "    grippe_input_train = create_exogenous_input(model=grippe_model, initial_input=grippe_train_scaled.iloc[0], seasonal_input=grippe_train_seasonal_scaled, n_steps=len(grippe_y_train_scaled))\n",
    "\n",
    "\n",
    "\n",
    "    # Extract training columns and output variable from dataframe\n",
    "    flu_training_cols = [col for col in df_lagged.columns if ('incValue' in col) and ('lag' in col) and ('_helper' not in col)] # Select cols to include as features\n",
    "    X_train = df_lagged_train[flu_training_cols]\n",
    "    y_train = df_lagged_train['incValue']\n",
    "    X_test = df_lagged_test[flu_training_cols]\n",
    "    y_test = df_lagged_test['incValue']\n",
    "\n",
    "    # Columns required for rolling of seasonal lag in iterative autoregressive forecast\n",
    "    flu_seasonal_col = [col for col in df_lagged.columns if ('incValue' in col) and ('_helper' in col)] # Select cols to include as features\n",
    "    X_train_seasonal = df_lagged_train[flu_seasonal_col]\n",
    "    X_test_seasonal = df_lagged_test[flu_seasonal_col]\n",
    "\n",
    "    # Create combined data to fit transform on all available historical lags in training set\n",
    "    oldest_lags = X_train.iloc[0, 1:].values.reshape(1, -1) # Take the first row of X_train (the oldest lags)\n",
    "    combined_data = np.vstack((y_train.values.reshape(-1, 1), oldest_lags.T)) # Concatenate y_train_cv with the oldest lags\n",
    "\n",
    "    # Fit Yeo-Johnson Transform on combined data\n",
    "    pt = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "    stdscaler = StandardScaler()\n",
    "    combined_data_transformed = pt.fit_transform(combined_data)\n",
    "    stdscaler.fit(combined_data_transformed)\n",
    "\n",
    "    # Apply transform and scaling to train and test sets\n",
    "    y_train_transformed = pt.transform(y_train.values.reshape(-1, 1)).flatten()\n",
    "    y_test_transformed = pt.transform(y_test.values.reshape(-1, 1)).flatten()\n",
    "    X_train_transformed = X_train.apply(lambda x: pt.transform(x.values.reshape(-1, 1)).flatten())\n",
    "    X_test_transformed = X_test.apply(lambda x: pt.transform(x.values.reshape(-1, 1)).flatten())\n",
    "    X_train_seasonal_trans = pt.transform(X_train_seasonal.values.reshape(-1, 1)).flatten()\n",
    "    X_test_seasonal_trans = pt.transform(X_test_seasonal.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Apply StandardScaler\n",
    "    y_train_scaled = stdscaler.transform(y_train_transformed.reshape(-1, 1)).flatten()\n",
    "    y_test_scaled = stdscaler.transform(y_test_transformed.reshape(-1, 1)).flatten()\n",
    "    X_train_scaled = X_train_transformed.apply(lambda x: stdscaler.transform(x.values.reshape(-1, 1)).flatten())\n",
    "    X_test_scaled = X_test_transformed.apply(lambda x: stdscaler.transform(x.values.reshape(-1, 1)).flatten())\n",
    "    X_train_seasonal_scaled = stdscaler.transform(X_train_seasonal_trans.reshape(-1, 1)).flatten()\n",
    "    X_test_seasonal_scaled = stdscaler.transform(X_test_seasonal_trans.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Initialize the final model configuration\n",
    "    final_model = MLPRegressor(max_iter=2000, \n",
    "                        random_state=42, \n",
    "                        solver='adam', \n",
    "                        activation=best_activation, \n",
    "                        hidden_layer_sizes=(best_hidden_layers), \n",
    "                        alpha=best_alpha, \n",
    "                        batch_size=best_batch_size, \n",
    "                        learning_rate_init=best_learning_rate)\n",
    "\n",
    "    # Train final model\n",
    "    final_model.fit(np.concatenate((X_train_scaled.values, grippe_train_scaled.values[:, :best_exogenous_lag], grippe_train_scaled.values[:, -1:]), axis=1), y_train_scaled)\n",
    "\n",
    "    # Forecast for the length of the test set\n",
    "    forecasts = iterative_forecast(model=final_model, \n",
    "                                initial_input=X_test_scaled.iloc[0], \n",
    "                                seasonal_input=X_test_seasonal_scaled, \n",
    "                                exogenous_forecasts=np.concatenate((grippe_input[:, :best_exogenous_lag], grippe_input[:, -1:]), axis=1), \n",
    "                                n_steps=len(y_test_scaled))\n",
    "\n",
    "    y_hat_train = iterative_forecast(model=final_model, \n",
    "                                    initial_input=X_train_scaled.iloc[0], \n",
    "                                    seasonal_input=X_train_seasonal_scaled, \n",
    "                                    exogenous_forecasts=np.concatenate((grippe_input_train[:, :best_exogenous_lag], grippe_input_train[:, -1:]), axis=1), \n",
    "                                    n_steps=len(y_train_scaled))\n",
    "\n",
    "    rmse = mean_squared_error(y_test_scaled, forecasts, squared=False)\n",
    "    print(rmse)\n",
    "    forecasts = stdscaler.inverse_transform(forecasts.reshape(-1, 1))\n",
    "    forecasts = pt.inverse_transform(forecasts.reshape(-1, 1))\n",
    "    y_hat_train = stdscaler.inverse_transform(y_hat_train.reshape(-1, 1))\n",
    "    y_hat_train = pt.inverse_transform(y_hat_train.reshape(-1, 1))\n",
    "\n",
    "    # Evaluate the forecasts against the actual y_test values\n",
    "    rmse = mean_squared_error(y_test, forecasts, squared=False)\n",
    "\n",
    "    # Plot the results\n",
    "    fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "    # Plot the true values\n",
    "    # ax.plot(plot['incValue'])\n",
    "\n",
    "    ax.plot(df_lagged['incValue'], label=\"True Train\", alpha=1, color='lightblue')\n",
    "    ax.plot(y_test.index, y_test, label=\"True Test\", alpha=0.8, color='blue')\n",
    "    ax.plot(y_test.index, forecasts, label='Predictions', alpha=0.7, color='red', linestyle='--')\n",
    "    ax.plot(y_train.index, y_hat_train, label='Prediction on Train', alpha=0.7, color='grey', linestyle='--')\n",
    "\n",
    "\n",
    "    # Add labels and legend\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"Incidence\")\n",
    "    ax.set_title(f'Nr: {rank_nr}, RMSE: {rmse}, auto_lag: {best_lag}, google_lag: {best_grippe_lag}, hidden layers: {best_hidden_layers}, alpha: {best_alpha:.4f}, learning rate: {best_learning_rate:.6f}', fontsize=10)\n",
    "    ax.legend()\n",
    "    rank_nr += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Discussion</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen in the plots below, including the forecasted lags from the Google search trend data did not help our predictions on the test set and in many of the cases above resulted in much worse predictions. This may be due to an accumulation of errors in the iterative procedure from both models. Further tuning is likely required but was limited by computational and time constraints in this study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
