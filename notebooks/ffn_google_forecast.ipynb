{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "flu = pd.read_csv('../data/INFLUENZA_sentinella/data.csv')\n",
    "weather = pd.read_csv('../data/weather/reg_weather.csv')\n",
    "google_flu = pd.read_csv('../data/google_search_trend/reg_google_grippe.csv')\n",
    "google_symptoms = pd.read_csv('../data/google_search_trend/reg_google_fieber_husten.csv')\n",
    "pop = pd.read_csv('../data/pop_data_cantons/weekly_imputed_pop_data_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Disclaimer</h2>\n",
    "\n",
    "#### This notebook was only used to tune the autoregressive neural network on the google data for the search term flu. The configurations were subsequently used to forecast the google data within the combined model in the notebook \"ffn_model_exogenous\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data Consolidation</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Extract relevant data from BAG dataset on weekly flu incidence</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe for regional observations, no differentiation between sex or age\n",
    "flu_reg = flu.query('georegion_type == \"sentinella_region\" and agegroup == \"all\" and sex == \"all\"').copy()\n",
    "\n",
    "# Drop rows for georegion \"unknown\", which only contain NaNs using mask\n",
    "flu_reg = flu_reg[~(flu_reg['georegion'] == 'unknown')]\n",
    "\n",
    "# Select columns required for analysis\n",
    "selected_cols = ['temporal', 'georegion', 'incValue', 'value']\n",
    "flu_reg = flu_reg[selected_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Convert dates and format of Google-Trend data for subsequent merging</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align time-indexes of google data and flu data using created date to iso-calendar week dict\n",
    "with open('date_dict.json', 'r') as f:\n",
    "    date_dict = json.load(f)\n",
    "\n",
    "# Create new column 'Woche' containing iso-calendar weeks for google-trend dates \n",
    "google_flu['Woche'] = google_flu['Woche'].apply(lambda x: date_dict[x]) \n",
    "google_symptoms['Woche'] = google_symptoms['Woche'].apply(lambda x: date_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape google_flu from wide to long to enable merging on date and region \n",
    "google_flu = google_flu.melt(id_vars=['Woche'], var_name='region_query', value_name='search_activity')\n",
    "\n",
    "# Separate region and query information from header into separate rows\n",
    "google_flu['region'] = google_flu['region_query'].apply(lambda x: \"_\".join(x.split('_')[:2]))\n",
    "google_flu['query'] = google_flu['region_query'].apply(lambda x: \"_\".join(x.split('_')[2:]))\n",
    "google_flu.drop(columns='region_query', inplace=True) # Drop superfluous region_query column\n",
    "\n",
    "# Reshape dataframe to get separate columns for each variable\n",
    "google_flu = google_flu.pivot(index=['Woche', 'region'], columns='query', values='search_activity').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Repeat above process for second google dataset containing data on symptom queries\n",
    "# Reshape google_flu from wide to long\n",
    "google_symptoms = google_symptoms.melt(id_vars=['Woche'], var_name='region_query', value_name='search_activity')\n",
    "\n",
    "# Separate region and query information from header into separate rows\n",
    "google_symptoms['region'] = google_symptoms['region_query'].apply(lambda x: \"_\".join(x.split('_')[:2]))\n",
    "google_symptoms['query'] = google_symptoms['region_query'].apply(lambda x: \"_\".join(x.split('_')[2:]))\n",
    "google_symptoms.drop(columns='region_query', inplace=True) # Drop superfluous region_query column\n",
    "\n",
    "# Reshape dataframe to get separate columns for each variable\n",
    "google_symptoms = google_symptoms.pivot(index=['Woche', 'region'], columns='query', values='search_activity').reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Convert date format of weather data for merging</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dates to 'YYYY-Www' ISO week format\n",
    "dates = weather.date.values\n",
    "iso_week_dates = [datetime.strptime(date, '%Y-%m-%d').isocalendar()[:2] for date in dates]\n",
    "iso_week_dates = [f'{year}-W{week:02d}' for year, week in iso_week_dates]\n",
    "weather['date'] = iso_week_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Merge datasets on date and region</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = pd.merge(flu_reg, weather, how='left', left_on=['temporal', 'georegion'], right_on=['date', 'region']).sort_values(by=['georegion', 'temporal'])\n",
    "merged_google = pd.merge(google_flu, google_symptoms, how='inner', on=['region', 'Woche'])\n",
    "merged_data = pd.merge(merged_data, merged_google, how='left', left_on=['georegion', 'temporal'], right_on=['region', 'Woche'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.drop(columns=['region_x', 'region_y', 'date', 'Woche'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Convert ISO calendar-weeks to Gregorian calendar (format 'YYYY-MM-DD')</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Convert ISO calendar-weeks to gregorian dates\n",
    "# Functions based on answer by Ben James: <https://stackoverflow.com/questions/304256/whats-the-best-way-to-find-the-inverse-of-datetime-isocalendar>\n",
    "def iso_year_start(iso_year):\n",
    "    \"The gregorian calendar date of the first day of the given ISO year\"\n",
    "    fourth_jan = datetime.date(iso_year, 1, 4)\n",
    "    delta = datetime.timedelta(fourth_jan.isoweekday()-1)\n",
    "    return fourth_jan - delta \n",
    "\n",
    "def iso_to_gregorian(iso_year, iso_week, iso_day):\n",
    "    \"Gregorian calendar date for the given ISO year, week and day\"\n",
    "    year_start = iso_year_start(iso_year)\n",
    "    return year_start + datetime.timedelta(days=iso_day-1, weeks=iso_week-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract week number and year from date column in ISO calendar week \n",
    "week_pattern = r'W(\\d{1,2})' # RegEx pattern to extract week nr without trailing zero\n",
    "merged_data['week_number'] = merged_data['temporal'].str.extract(week_pattern).astype(int)\n",
    "merged_data['year'] = merged_data['temporal'].apply(lambda x: x.split('-')[0])\n",
    "merged_data['year'] = pd.to_numeric(merged_data['year']) # Convert from string to numeric\n",
    "\n",
    "# Convert from iso-calendar week to gregorian dates (format: YYYY-MM-DD)\n",
    "merged_data['date'] = list(map(lambda year, week: iso_to_gregorian(year, week, 4), merged_data['year'], merged_data['week_number']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect resulting dataframe\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Data exploration</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display descriptive statistics\n",
    "merged_data['incValue'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display missing values in reported flu incidence across regions\n",
    "merged_data[merged_data['incValue'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values in March 2020 linearly\n",
    "merged_data['incValue'].interpolate(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Inspect incidence of consultations for influenza-like-diseases over time</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary for cantons within each region\n",
    "region_to_ct = {'region_1': ['Genf', 'Neuenburg', 'Waadt', 'Wallis'], \n",
    "           'region_2': ['Bern', 'Freiburg', 'Jura'], \n",
    "           'region_3': ['Aargau', 'Basel-Landschaft', 'Basel-Stadt', 'Solothurn'], \n",
    "           'region_4': ['Luzern', 'Nidwalden', 'Obwalden', 'Schwyz', 'Uri', 'Zug'], \n",
    "           'region_5': ['Appenzell_Innerrhoden', 'Appenzell_Ausserrhoden', 'Glarus', 'Sankt_Gallen', 'Schaffhausen', 'Thurgau', 'Zürich'], \n",
    "           'region_6': ['Graubünden', 'Tessin']}\n",
    "\n",
    "# Plot the regional incidence value for the provided timeframe\n",
    "\n",
    "#  Set plot style\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Create subplots for each region\n",
    "fig, ax = plt.subplots(6, figsize=(10, 15))  # Adjusted figure size for better spacing\n",
    "fig.suptitle('Weekly incidence of consultations for influenza-like-diseases per region from 2013-2023', fontsize=12)\n",
    "\n",
    "# Adjust the spacing of the subplots\n",
    "fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "fig.subplots_adjust(hspace=0.5)  # Adjust horizontal space between plots\n",
    "\n",
    "for i in range(1, 7):\n",
    "    ax[i-1].axvline(pd.Timestamp('2020-01-01'), linestyle='--', color='grey', lw=1, alpha=.7)\n",
    "    ax[i-1].plot(merged_data.set_index('date').loc[merged_data.set_index('date')['georegion'] == f\"region_{i}\", 'incValue'])\n",
    "    ax[i-1].set_title(f\"Region {i}: \\n{region_to_ct[f'region_{i}']}\", fontsize=10)\n",
    "\n",
    "    if i == 5:\n",
    "    # Change the color and line width of the spines for region 5\n",
    "        for spine in ax[i-1].spines.values():\n",
    "            spine.set_edgecolor('black')\n",
    "            spine.set_linewidth(2)\n",
    "            spine.set_visible(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Modelling</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lagged_features(df, column, number_of_lags, seasonal_lags=None):\n",
    "    # Copy the original DataFrame to avoid modifying it\n",
    "    df_lagged = df.copy()\n",
    "\n",
    "    # Generate regular lagged features\n",
    "    for lag in range(1, number_of_lags + 1):\n",
    "        df_lagged[f'lag_{lag}'] = df_lagged[column].shift(lag)\n",
    "\n",
    "    # Generate seasonal lags\n",
    "    if seasonal_lags is not None:\n",
    "        for season_lag in seasonal_lags:\n",
    "            df_lagged[f'seasonal_lag_{season_lag}_helper'] = df_lagged[column].shift(season_lag-1)\n",
    "            df_lagged[f'seasonal_lag_{season_lag}'] = df_lagged[column].shift(season_lag)\n",
    "\n",
    "    return df_lagged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoregressive_iterative_forecast(model, initial_input, seasonal_input, n_steps):\n",
    "    \"\"\"\n",
    "    Perform iterative forecasting using an autoregressive model.\n",
    "\n",
    "    Args:\n",
    "        model: Trained autoregressive model (e.g., MLPRegressor).\n",
    "        initial_input: The initial input features (e.g., the last observation from the training set).\n",
    "        n_steps: Number of future time steps to forecast.\n",
    "\n",
    "    Returns:\n",
    "        A list of forecasts, one for each future time step.\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    current_input = initial_input.copy()\n",
    "    current_input = np.array(current_input)\n",
    "    seasonal_input = np.array(seasonal_input)\n",
    "    # print(f'Seasonal lags: {seasonal_input}')\n",
    "    forecasts = []\n",
    "\n",
    "    for _ in range(n_steps):\n",
    "        # Predict the next step\n",
    "        # print(f'Current_input start: {current_input}\\n')\n",
    "        # print(len(current_input))\n",
    "        next_step_pred = model.predict(current_input.reshape(1, -1))[0]\n",
    "        # print(f'\\nWeek {i+1}\\n')  \n",
    "        # print(f'Pred: {next_step_pred}')  \n",
    "        forecasts.append(next_step_pred)\n",
    "        # print(f'Forecasts: {forecasts}')\n",
    "        \n",
    "        # Update the current input to include the new prediction\n",
    "        # Roll all lags except the last one (seasonal lag)\n",
    "        current_input[:-1] = np.roll(current_input[:-1], 1)\n",
    "        # print(f'Current_input after roll: {current_input}\\n')\n",
    "        current_input[0] = next_step_pred\n",
    "        \n",
    "        # Update the seasonal lag (52-period lag)\n",
    "        if i < 52:\n",
    "            # Use the actual seasonal lag value for the first 52 weeks\n",
    "            current_input[-1] = seasonal_input[i]\n",
    "        else:\n",
    "            # Use forecasted value for the seasonal lag after 52 weeks\n",
    "            current_input[-1] = forecasts[i - 52]\n",
    "        # This needs to be handled based on your specific logic\n",
    "        # For example, fetching the value from 52 periods ago or some other logic\n",
    "        \n",
    "        # print(f'Current_input with next_step_pred: {current_input}')\n",
    "        # print(f'\\n {60*\"#\"}\\n')\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return np.array(forecasts)\n",
    "\n",
    "# training_cols = [col for col in df_lagged.columns if ('lag_' in col) and ('_helper' not in col)]\n",
    "# X_train_cv_scaled = df_lagged[training_cols]\n",
    "# print(X_train_cv_scaled.iloc[0], f'Length:{len(X_train_cv_scaled.iloc[0])}')\n",
    "# print(df_lagged['seasonal_lag_52_helper'].iloc[:52])\n",
    "# prediction = autoregressive_iterative_forecast(model, X_train_cv_scaled.iloc[0], df_lagged[['seasonal_lag_52_helper']], 53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "data = merged_data.loc[(merged_data['georegion'] == \"region_5\") & (merged_data['date'].apply(lambda x: x.year < 2020))]\n",
    "# Split the data\n",
    "target = 'Grippe'\n",
    "y = data[target]\n",
    "split = int(len(y) * 0.8)\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "i = 0\n",
    "scores_df = pd.DataFrame(columns=['RMSE', 'lags', 'seasonal_lags', 'hidden_layers', 'alpha', 'batch_size', 'activation', 'learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# Suppress convergence warnings\n",
    "# warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "\n",
    "# Define parameter configurations to assess\n",
    "lags = 52 # Autoregressive lags to consider\n",
    "hidden_layer_sizes = [(16, 16), (16, 16, 16)]\n",
    "alphas = np.linspace(0.1, 0.3, num=100) # Regularization parameter\n",
    "batch_size = 32\n",
    "learning_rates = np.logspace(-3, -4, 100)\n",
    "activations = ['relu']\n",
    "seasonal = [52]\n",
    "\n",
    "# Extract data\n",
    "data = merged_data.loc[(merged_data['georegion'] == \"region_5\") & (merged_data['date'].apply(lambda x: x.year < 2020))]\n",
    "# Split the data\n",
    "y = data[target]\n",
    "split = int(len(y) * 0.8)\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "i = 0\n",
    "scores_df = pd.DataFrame(columns=['RMSE', 'lags', 'seasonal_lags', 'hidden_layers', 'alpha', 'batch_size', 'activation', 'learning_rate'])\n",
    "\n",
    "# Randomized search of hyperparameter configurations\n",
    "random.seed(42)\n",
    "iterations = 1000\n",
    "\n",
    "for _ in range(iterations):\n",
    "    # Randomly select hyperparameters\n",
    "    lag = random.randint(1, lags)\n",
    "    activation = random.choice(activations)\n",
    "    learning_rate = random.choice(learning_rates)\n",
    "    alpha = random.choice(alphas)\n",
    "    hidden_layer_size = random.choice(hidden_layer_sizes)\n",
    "\n",
    "    # Keep track of configurations and cv scores\n",
    "    model = MLPRegressor(max_iter=2000, \n",
    "                        random_state=42, \n",
    "                        solver='adam', \n",
    "                        activation=activation, \n",
    "                        hidden_layer_sizes=hidden_layer_size, \n",
    "                        alpha=alpha, \n",
    "                        batch_size=batch_size, \n",
    "                        learning_rate_init=learning_rate,\n",
    "                        warm_start=False, \n",
    "                        early_stopping=True)\n",
    "    scores = []\n",
    "    \n",
    "    # Create lagged features based on the whole y_train\n",
    "    df_lagged = create_lagged_features(pd.DataFrame(y_train, columns=[target]), column=target, number_of_lags=lag, seasonal_lags=seasonal)\n",
    "    df_lagged.dropna(inplace=True)\n",
    "    \n",
    "    training_cols = [col for col in df_lagged.columns if ('lag_' in col) and ('_helper' not in col)]\n",
    "    X = df_lagged[training_cols]\n",
    "    X_seasonal = df_lagged['seasonal_lag_52_helper']\n",
    "    y = df_lagged[target]\n",
    "    # print(X)\n",
    "    # print(y)\n",
    "    \n",
    "    val_index = range(len(y) - 52, len(y))\n",
    "    train_index = range(0, len(y) - 52)\n",
    "        \n",
    "    y_train_cv, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    X_train_cv, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    X_seasonal_train, X_seasonal_val = X_seasonal.iloc[train_index], X_seasonal.iloc[val_index]\n",
    "\n",
    "    # Take the first row of X_train_cv (the oldest lags)\n",
    "    oldest_lags = X_train_cv.iloc[0, 1:].values.reshape(1, -1)\n",
    "\n",
    "    # Concatenate y_train_cv with the oldest lags\n",
    "    combined_data = np.vstack((y_train_cv.values.reshape(-1, 1), oldest_lags.T))\n",
    "\n",
    "    # Fit the PowerTransformer and StandardScaler on the available lags in the training data (incl. lags in first row of lag df_train)\n",
    "    pt = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "    stdscaler = StandardScaler()\n",
    "    combined_data_transformed = pt.fit_transform(combined_data)\n",
    "    stdscaler.fit(combined_data_transformed)\n",
    "    \n",
    "    # Apply Transform to the entire y_train_cv\n",
    "    y_train_cv_transformed = pt.transform(y_train_cv.values.reshape(-1, 1)).flatten()\n",
    "    y_val_transformed = pt.transform(y_val.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Apply the PowerTransformer to each lagged feature in X_train_cv and X_val\n",
    "    X_train_cv_transformed = X_train_cv.apply(lambda column: pt.transform(column.values.reshape(-1, 1)).flatten())\n",
    "    X_val_transformed = X_val.apply(lambda column: pt.transform(column.values.reshape(-1, 1)).flatten())\n",
    "    X_seasonal_train_trans = pt.transform(X_seasonal_train.values.reshape(-1, 1)).flatten()\n",
    "    X_seasonal_val_trans = pt.transform(X_seasonal_val.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "    \n",
    "    # Apply StandardScaler()\n",
    "    y_train_cv_scaled = stdscaler.transform(y_train_cv_transformed.reshape(-1, 1)).flatten()\n",
    "    y_val_scaled = stdscaler.transform(y_val_transformed.reshape(-1, 1)).flatten()\n",
    "    X_train_cv_scaled = X_train_cv_transformed.apply(lambda column: stdscaler.transform(column.values.reshape(-1, 1)).flatten())\n",
    "    X_val_scaled = X_val_transformed.apply(lambda column: stdscaler.transform(column.values.reshape(-1, 1)).flatten())\n",
    "    X_seasonal_train_scaled = stdscaler.transform(X_seasonal_train_trans.reshape(-1, 1)).flatten()\n",
    "    X_seasonal_val_scaled = stdscaler.transform(X_seasonal_val_trans.reshape(-1, 1)).flatten()\n",
    "\n",
    "    ######################\n",
    "    # NOTE: PLOT VALIDATION AND TRAINING LOSSES - Adjust max_iter to 1 and set warm_start = True to enable\n",
    "\n",
    "    # training_losses = []\n",
    "    # validation_losses = []\n",
    "\n",
    "    # for epoch in range(1000):  # Adjust the number of epochs as needed\n",
    "    #     model.fit(X_train_cv_scaled.values, y_train_cv_scaled)\n",
    "\n",
    "    #     # Store training loss from the last iteration\n",
    "    #     training_losses.append(model.loss_curve_[-1])\n",
    "\n",
    "    #     # Compute and store validation loss\n",
    "    #     val_predictions = model.predict(X_val_scaled.values)\n",
    "    #     val_loss = mean_squared_error(y_val_scaled, val_predictions)\n",
    "    #     validation_losses.append(val_loss)\n",
    "    \n",
    "    # if fold == 2:\n",
    "    #     plt.plot(training_losses, label='Training Loss')\n",
    "    #     # If you have validation loss, plot it here\n",
    "    #     plt.plot(validation_losses, label='Validation Loss')\n",
    "\n",
    "    #     plt.title('Learning Curve')\n",
    "    #     plt.xlabel('Epochs')\n",
    "    #     plt.ylabel('Loss')\n",
    "    #     plt.title(f'Lags: {lag}, Learning-rate: {learning_rate}, alpha: {alpha}, hidden layers: {hidden_layer_size}')\n",
    "    #     plt.legend()\n",
    "    #     plt.show()\n",
    "\n",
    "    #######################\n",
    "\n",
    "    # Fit model\n",
    "    model.fit(X_train_cv_scaled.values, y_train_cv_scaled)\n",
    "    # loss_values = model.loss_curve_\n",
    "    \n",
    "    # Make iterative forecasts (NOTE: train and val splits are numpy arrays, seasonal helper columns necessary for updating of seasonal lag)\n",
    "    # print(f'X_val_scaled: {X_val_scaled}')\n",
    "    # print(f'X_val_scaled: {X_val_scaled.iloc[0]}')\n",
    "    prediction = autoregressive_iterative_forecast(model, X_val_scaled.iloc[0], X_seasonal_val_scaled, len(y_val_scaled))\n",
    "    y_hat_train = autoregressive_iterative_forecast(model, X_train_cv_scaled.iloc[0], X_seasonal_train_scaled, len(y_train_cv_scaled))\n",
    "    prediction = np.array(prediction).flatten()\n",
    "    y_hat_train = np.array(y_hat_train).flatten()\n",
    "\n",
    "    rmse = mean_squared_error(y_val_scaled, prediction, squared=False)\n",
    "\n",
    "    # NOTE: UNCOMMENT FOR ORIGINAL SCALE PLOTTING AND RMSE - Reverse transform to plot original scale train-validation results\n",
    "    prediction = stdscaler.inverse_transform(prediction.reshape(-1, 1))\n",
    "    prediction = pt.inverse_transform(prediction.reshape(-1, 1))\n",
    "    y_hat_train = stdscaler.inverse_transform(y_hat_train.reshape(-1, 1)) \n",
    "    y_hat_train = pt.inverse_transform(y_hat_train.reshape(-1, 1)) \n",
    "    \n",
    "    rmse = mean_squared_error(y_val, prediction, squared=False)\n",
    "\n",
    "    scores.append(rmse)\n",
    "    \n",
    "    ## PLOT SCALED PREDICTIONS - Ensure to comment out the inverse scaling just above\n",
    "    # plt.plot(range(len(y_train_cv_scaled)), y_train_cv_scaled, label='Training Actual', color='blue')\n",
    "    # plt.plot(range(len(y_train_cv_scaled), len(y_train_cv_scaled) + len(y_val_scaled)), y_val_scaled, label='Validation Actual', color='blue')\n",
    "    # plt.plot(range(len(y_train_cv_scaled), len(y_train_cv_scaled) + len(y_val_scaled)), prediction, label='Validation Predicted', color='red', linestyle='--')\n",
    "    # plt.plot(range(len(y_train_cv_scaled)), y_hat_train, label='Train-Set Prediction', color='orange', linestyle='--')\n",
    "    \n",
    "    ## PLOT ORIGINAL SCALE\n",
    "    # plt.plot(range(len(y_train_cv)), y_train_cv, label='Training Actual', color='blue')\n",
    "    # plt.plot(range(len(y_train_cv), len(y_train_cv) + len(y_val)), y_val, label='Validation Actual', color='blue')\n",
    "    # plt.plot(range(len(y_train_cv), len(y_train_cv) + len(y_val)), prediction, label='Validation Predicted', color='red', linestyle='--')\n",
    "    # plt.plot(range(len(y_train_cv)), y_hat_train, label='Train-Set Prediction', color='orange', linestyle='--')\n",
    "    \n",
    "    # plt.title(f'Nr: {i}; Lag: {lag}; alpha: {alpha}; hidden layers: {hidden_layer_size}')\n",
    "    # plt.xlabel('Time')\n",
    "    # plt.ylabel('Scaled Value')\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "    ########################\n",
    "\n",
    "    # Fill in parameters and score for each configuration \n",
    "    scores_df.loc[i, 'lags'] = lag\n",
    "    scores_df.loc[i, 'seasonal_lags'] = seasonal\n",
    "    scores_df.loc[i, 'hidden_layers'] = hidden_layer_size\n",
    "    scores_df.loc[i, 'alpha'] = alpha\n",
    "    scores_df.loc[i, 'batch_size'] = batch_size\n",
    "    scores_df.loc[i, 'activation'] = activation\n",
    "    scores_df.loc[i, 'learning_rate'] = learning_rate\n",
    "    scores_df.loc[i, 'RMSE'] = np.mean(scores)\n",
    "    print(f'{i}/{iterations}: {(i/iterations)*100:.2f}%')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df['RMSE'] = pd.to_numeric(scores_df['RMSE'])\n",
    "# Best parameters and score\n",
    "best_config_index = scores_df['RMSE'].idxmin()  # This gets the index of the minimum RMSE\n",
    "best_config = scores_df.loc[best_config_index]  # Use the index to access the row\n",
    "best_score = best_config['RMSE']\n",
    "print(f\"Best parameters: {best_config}\")\n",
    "print(f\"Best score (RMSE): {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df.sort_values(by='RMSE').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_nr = 1\n",
    "for index_nr in scores_df.sort_values(by='RMSE').head(10).index:\n",
    "    \n",
    "    best_config = scores_df.loc[index_nr]\n",
    "    \n",
    "    best_lag = best_config.values[1]\n",
    "    best_seasonal_lag = best_config.values[2]\n",
    "    best_hidden_layers = best_config.values[3]\n",
    "    best_alpha = best_config.values[4]\n",
    "    best_batch_size = best_config.values[5]\n",
    "    best_activation = best_config.values[6]\n",
    "    best_learning_rate = best_config.values[7]\n",
    "\n",
    "\n",
    "    # Extract data\n",
    "    data = merged_data.loc[(merged_data['georegion'] == \"region_5\") & (merged_data['date'].apply(lambda x: x.year < 2020))]\n",
    "    # Split the data\n",
    "    y = data[target]\n",
    "\n",
    "    # Create lagged features based on the whole y\n",
    "    df_lagged = create_lagged_features(pd.DataFrame(y, columns=[target]), column=target, number_of_lags=best_lag, seasonal_lags=best_seasonal_lag)\n",
    "\n",
    "    split = int(len(y) * 0.8)\n",
    "    # NOTE: SPLIT BEFORE DROPPING TO AVOID DATA LEAKAGE\n",
    "    df_lagged_train = df_lagged.iloc[:split]\n",
    "    df_lagged_train = df_lagged_train.dropna()\n",
    "    df_lagged_test = df_lagged.iloc[split:]\n",
    "\n",
    "    # Extract training columns and output variable from dataframe\n",
    "    training_cols = [col for col in df_lagged.columns if ('lag_' in col) and ('helper' not in col)]\n",
    "    X_train = df_lagged_train[training_cols]\n",
    "    y_train = df_lagged_train[target]\n",
    "    X_test = df_lagged_test[training_cols]\n",
    "    y_test = df_lagged_test[target]\n",
    "\n",
    "    # Columns required for rolling of seasonal lag in iterative autoregressive forecast\n",
    "    X_train_seasonal = df_lagged_train['seasonal_lag_52_helper']\n",
    "    X_test_seasonal = df_lagged_test['seasonal_lag_52_helper']\n",
    "\n",
    "    # Create combined data to fit transform on all available historical lags in training set\n",
    "    oldest_lags = X_train.iloc[0, 1:].values.reshape(1, -1) # Take the first row of X_train_cv (the oldest lags)\n",
    "    combined_data = np.vstack((y_train.values.reshape(-1, 1), oldest_lags.T)) # Concatenate y_train_cv with the oldest lags\n",
    "\n",
    "    # Fit Yeo-Johnson Transform on combined data\n",
    "    pt = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "    stdscaler = StandardScaler()\n",
    "    combined_data_transformed = pt.fit_transform(combined_data)\n",
    "    stdscaler.fit(combined_data_transformed)\n",
    "\n",
    "    # Apply transform and scaling to train and test sets\n",
    "    y_train_transformed = pt.transform(y_train.values.reshape(-1, 1)).flatten()\n",
    "    y_test_transformed = pt.transform(y_test.values.reshape(-1, 1)).flatten()\n",
    "    X_train_transformed = X_train.apply(lambda x: pt.transform(x.values.reshape(-1, 1)).flatten())\n",
    "    X_test_transformed = X_test.apply(lambda x: pt.transform(x.values.reshape(-1, 1)).flatten())\n",
    "    # X_train_transformed = X_train.apply(lambda x: pt.transform(x))\n",
    "    # X_test_transformed = X_test.apply(lambda x: pt.transform(x.values.reshape(-1, 1))).flatten()\n",
    "    X_train_seasonal_trans = pt.transform(X_train_seasonal.values.reshape(-1, 1)).flatten()\n",
    "    X_test_seasonal_trans = pt.transform(X_test_seasonal.values.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Apply StandardScaler\n",
    "    y_train_scaled = stdscaler.transform(y_train_transformed.reshape(-1, 1)).flatten()\n",
    "    y_test_scaled = stdscaler.transform(y_test_transformed.reshape(-1, 1)).flatten()\n",
    "    X_train_scaled = X_train_transformed.apply(lambda x: stdscaler.transform(x.values.reshape(-1, 1)).flatten())\n",
    "    X_test_scaled = X_test_transformed.apply(lambda x: stdscaler.transform(x.values.reshape(-1, 1)).flatten())\n",
    "    X_train_seasonal_scaled = stdscaler.transform(X_train_seasonal_trans.reshape(-1, 1)).flatten()\n",
    "    X_test_seasonal_scaled = stdscaler.transform(X_test_seasonal_trans.reshape(-1, 1)).flatten()\n",
    "\n",
    "    # Initialize the final model configuration\n",
    "    final_model = MLPRegressor(max_iter=2000, \n",
    "                        random_state=42, \n",
    "                        solver='adam', \n",
    "                        activation=best_activation, \n",
    "                        hidden_layer_sizes=(best_hidden_layers), \n",
    "                        alpha=best_alpha, \n",
    "                        batch_size=best_batch_size, \n",
    "                        learning_rate_init=best_learning_rate)\n",
    "\n",
    "    # Train final model\n",
    "    final_model.fit(X_train_scaled.values, y_train_scaled) \n",
    "\n",
    "    # Forecast for the length of the test set\n",
    "    forecasts = autoregressive_iterative_forecast(final_model, X_test_scaled.iloc[0], X_test_seasonal_scaled,len(y_test_scaled))\n",
    "    y_hat_train = autoregressive_iterative_forecast(final_model, X_train_scaled.iloc[0], X_train_seasonal_scaled, len(y_train))\n",
    "\n",
    "    forecasts = stdscaler.inverse_transform(forecasts.reshape(-1, 1))\n",
    "    forecasts = pt.inverse_transform(forecasts.reshape(-1, 1))\n",
    "    y_hat_train = stdscaler.inverse_transform(y_hat_train.reshape(-1, 1))\n",
    "    y_hat_train = pt.inverse_transform(y_hat_train.reshape(-1, 1))\n",
    "\n",
    "    # Evaluate the forecasts against the actual y_test values\n",
    "    rmse = mean_squared_error(y_test, forecasts, squared=False)\n",
    "\n",
    "    # Plot the results\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "    # Plot the true values\n",
    "    # ax.plot(plot['incValue'])\n",
    "\n",
    "    ax.plot(df_lagged[target], label=\"True Train\", alpha=1, color='lightblue')\n",
    "    ax.plot(y_test.index, y_test, label=\"True Test\", alpha=0.8, color='blue')\n",
    "    ax.plot(y_test.index, forecasts, label='Predictions', alpha=0.7, color='red', linestyle='--')\n",
    "    ax.plot(y_train.index, y_hat_train, label='Prediction on Train', alpha=0.7, color='grey', linestyle='--')\n",
    "\n",
    "\n",
    "    # Add labels and legend\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"Incidence\")\n",
    "    ax.set_title(f'Nr: {rank_nr}, RMSE: {rmse}, lag: {best_lag}, hidden layers: {best_hidden_layers}, alpha: {best_alpha:.4f}, learning rate: {best_learning_rate:.6f}', fontsize=10)\n",
    "    ax.legend()\n",
    "    rank_nr += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
